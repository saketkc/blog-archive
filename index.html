<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Piddling Pertinent</title>
        <link rel="stylesheet" href="./theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="./">Piddling Pertinent </a></h1>
                <nav><ul>
                    <li><a href="./category/math-statistics-ml.html">math, statistics, ml</a></li>
                    <li><a href="./category/misc.html">misc</a></li>
                    <li><a href="./category/statistics.html">statistics</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="./svd-vs-mds-vs-pca.html">SVD v/s MDS v/s PCA</a></h1>
<footer class="post-info">
        <abbr class="published" title="2015-04-15T00:00:00+02:00">
                Published: Wed 15 April 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="./author/saket-choudhary.html">Saket Choudhary</a>
        </address>
<p>In <a href="./category/math-statistics-ml.html">math, statistics, ml</a>. </p>

</footer><!-- /.post-info --><p>Principle Component Analysis(PCA) is a relatively more famous
than Singular Value Decomposition(SVD) or Multidimensional Scaling(MDS).
When I was introduced to the latter two, I was utterly confused
trying to figure out what goes in where.</p>
<h2>SVD</h2>
<p>Let <span class="math">\(X_{mxn}\)</span> data matrix. For an easy to relate example, from bioinformatics,
let each row represent a gene, and each column represent single patient.</p>
<p>The rows thus give expression profile of gene across patients while the columns
represent the expression levels of different genes in each person.</p>
<p>Without loss of generality we assume <span class="math">\(m&gt;n\)</span> and <span class="math">\(rank(X)=r &lt;n\)</span></p>
<p>The singular value decomposition of <span class="math">\(X_{mxn}\)</span> is then given by:</p>
<div class="math">$$
X_{mxn} = U_{mxr}\sum_{rxr}V_{nxr}^T
$$</div>
<p><span class="math">\(U_{nxr}\)</span> consists of orthornormal eigen vectors of <span class="math">\(X^TX\)</span>(nxn)</p>
<p><span class="math">\(V_{mxr}\)</span> consists of orthornormal eigen vectors of <span class="math">\(XX^T\)</span>(mxm)</p>
<p><span class="math">\(\sum_{rxr}\)</span> denotes a diagonal matrix composed of eigen vectors of <span class="math">\(X^TX\)</span> arranged with the largest being located
at top left, least at bottom right.<span class="math">\(\sum_{ii} = \lambda_i\)</span> and <span class="math">\(\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \ldots \geq \lambda_r\)</span></p>
<p>Facts:</p>
<ul>
<li><span class="math">\(\lambda_i \geq 0\)</span> because <span class="math">\(X^TX\)</span> is a positive definite matrix: <span class="math">\(yX^TXy = (Xy)(Xy)^T &gt; 0\)</span> always [See: http://en.wikipedia.org/wiki/Positive-definite_matrix#Characterizations]</li>
</ul>
<h2>PCA</h2>
<p>Let <span class="math">\(X_c\)</span> denote the mean centered <span class="math">\(X\)</span>, i.e. <span class="math">\(X_c = I’_{mxm}X_{mxn}\)</span>
 where <span class="math">\(I’ = I-\frac{1}{m}\)</span> thus from each row we substract the ‘average row’ leading to $X_c4 being zer-mean centerered.</p>
<p>Def: Gram matrix : <span class="math">\(X_cX_c^T\)</span></p>
<p>Def: Covariance Matrix: <span class="math">\(X_c^TX_c\)</span></p>
<p>Let’s say you are given the original uncente</p>
<p>The most ‘common’ definition of PCA says : For a given set of <span class="math">\(n\)</span> dimensional vectors
<span class="math">\(x_1, x_2, x_3,x_4, \ldots x_m\)</span> the <span class="math">\(p&lt;n\)</span> principal components are those orthogonal
axes onto which the variance retained is maximized</p>
<p>and hence </p>
<p>Plotting/Running PCA
- Calculate <span class="math">\(X_cX_c^T\)</span> and calculate <span class="math">\(U\)</span> as the eigen vectors of this matrix. Retain the vectors corresponding
to top <span class="math">\(p=2\)</span> eigenvalues
-  Calculate project <span class="math">\(Y=U^TX\)</span></p>
<p>Given gram matrix of original data <span class="math">\(K\)</span>, to obtain gram matrix of mean centered data, cholesky decomposition is not required:</p>
<p><span class="math">\(K_c = (I-\frac{1}{n})K(I-\frac{1}{n}) = K-\frac{I}{n}K -K\frac{1}{n} \frac{I}{n}K\frac{I}{n}\)</span></p>
<p>Also realize:</p>
<p><span class="math">\(X_c = U \sum V^T\)</span></p>
<p>and <span class="math">\(UU^T=I\)</span> ; <span class="math">\(VV^T=I\)</span></p>
<p>Thus, <span class="math">\(K_c=X_cX_c^T = U\sum^2U^T\)</span></p>
<p>PCA is equivalent of doing a SVD: of <span class="math">\(X_c = U\sumV^T\)</span> and then using <span class="math">\(U\sum\)</span> as the principle components</p>
<h2>MDS</h2>
<p>Given distance matrix <span class="math">\(D_{ij}\)</span> of pairwise distances, what would PCA result into?
Assuming the distances were euclidean:</p>
<div class="math">$$
D_{ij}^2 = ||x_i-x_j||^2 = ||x_i - \overline{x} + \overline{x} -x_j||^2 = ||x_i-\overline{x}||^2 + ||x_j - \overline{x}|^2 -2\langle x_i-\overline{x}, x_j-\overline{x}\rangle
= ||x_i-\overline{x}||^2 + ||x_j - \overline{x}|^2 + 2[K_c]_{ij}
$$</div>
<p>
which can be cleverly rewritten as:
</p>
<div class="math">$$K_c = (I-\frac{1}{n})\frac{-D^2}{2}(I-\frac{1}{n}$$</div>
<p><span class="math">\(K_c\)</span> can now undergo SVD to obtain principal components. which is what happens in <a href="MDS.md">MDS</a> too.</p>
<h2>References</h2>
<p>[1] http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional/132731#132731</p>
<p>[2] SVD: http://infolab.stanford.edu/~ullman/mmds/ch11.pdf</p>
<p>[3] SVD in R: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition</p>
<p>[4] Multidimensional Scaling, Patrick J.F. Groenen∗ Michel van de Velden</p>
<p>[5] SVD and PCA: http://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca</p>
<p>[6] Intuitive Explaination of PCA: http://arxiv.org/pdf/1404.1100.pdf</p>
<p>[7] PCA: http://www3.cs.stonybrook.edu/~sael/teaching/cse549/Slides/CSE549_16.pdf</p>
<p>[8] PCA: http://www.math.ucsd.edu/~gptesler/283/slides/pca_f13-handout.pdf</p>
<p>[9] SVD: http://www.cs.wustl.edu/~zhang/teaching/cs517/Spring12/CourseProjects/SVD.pdf</p>
<p>[10] PCA v/s MDS: http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional</p>
<p>[11] Gaussian Lernels: https://shapeofdata.wordpress.com/2013/07/23/gaussian-kernels/</p>
<p>[12] Kernel PCA: http://sebastianraschka.com/Articles/2014_kernel_pca.html</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="./multidimensional-scaling.html" rel="bookmark"
                           title="Permalink to Multidimensional Scaling">Multidimensional Scaling</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-04-14T00:00:00+02:00">
                Published: Tue 14 April 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="./author/saket-choudhary.html">Saket Choudhary</a>
        </address>
<p>In <a href="./category/statistics.html">statistics</a>. </p>

</footer><!-- /.post-info -->                <p>MDS is a statistical technique to visualize dissimilarity
between points. The distances between two pointsin n-dimensions 
are visualized in 2 dimensions such that it represents
the distance in n-dimensions as far as possible.</p>
<p>It is important to note that, for MDS, we start of with a ‘distance’ matrix
and not ...</p>
                <a class="readmore" href="./multidimensional-scaling.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="./a-mod-b-is-olog-n.html" rel="bookmark"
                           title="Permalink to a mod b is O(log n)">a mod b is O(log n)</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-03-08T00:00:00+01:00">
                Published: Sun 08 March 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="./author/saket-choudhary.html">Saket Choudhary</a>
        </address>
<p>In <a href="./category/misc.html">misc</a>. </p>
<p>tags: <a href="./tag/number-theory.html">number theory</a> <a href="./tag/time-complexity.html">time complexity</a> </p>
</footer><!-- /.post-info -->                <p>Test inline math <span class="math">\(\frac{1}{3}\)</span>
Block math 
</p>
<div class="math">$$
\frac{1}{444}
X \Leftarrow Y
$$</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script ...</script>
                <a class="readmore" href="./a-mod-b-is-olog-n.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="./ba-ab-i.html" rel="bookmark"
                           title="Permalink to BA = AB = I">BA = AB = I</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2015-02-28T00:00:00+01:00">
                Published: Sat 28 February 2015
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="./author/saket-choudhary.html">Saket Choudhary</a>
        </address>
<p>In <a href="./category/misc.html">misc</a>. </p>
<p>tags: <a href="./tag/linear-algebra.html">linear algebra</a> <a href="./tag/matrix.html">matrix</a> <a href="./tag/norm.html">norm</a> </p>
</footer><!-- /.post-info -->                <p>To Prove: If </p>
<div class="math">$$A_{nxn}$$</div>
<p> and </p>
<div class="math">$$B_{nxn}$$</div>
<p> such that AB=I, then BA=I</p>
<div class="math">$$AB=I \implies Rank(A), Rank(B)=n$$</div>
<p>Reason: Rank(AB) </p>
<div class="math">$$\leq$$</div>
<p> min(Rank A, Rank B)</p>
<p>so B is a full rank matrix.
Now consider B=BI </p>
<div class="math">$$\implies$$</div>
<p> B-B(AB)=0 </p>
<div class="math">$$\implies$$</div>
<p> B-(BA ...</p>
                <a class="readmore" href="./ba-ab-i.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
                </section><!-- /#content -->
        <section id="extras" class="body">
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>