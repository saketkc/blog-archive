var tipuesearch = {"pages":[{"title":"Growth of NCBI's SRA","text":"The plots here visualize the growth of NCBI's Sequence Read Archive over the past 10+ years. See notebook","tags":"misc","url":"https://saket-choudhary.me/growth-of-ncbis-sra.html","loc":"https://saket-choudhary.me/growth-of-ncbis-sra.html"},{"title":"Fivethirtyeight Riddler - A curious case of colonies","text":"You are studying a new strain of bacteria, Riddlerium classicum (or R. classicum, as the researchers call it). Each R. classicum bacterium will do one of two things: split into two copies of itself or die. There is an 80 percent chance of the former and a 20 percent chance of the latter. If you start with a single R. classicum bacterium, what is the probability that it will lead to an everlasting colony (i.e., the colony will theoretically persist for an infinite amount of time)? Extra credit: Suppose that, instead of 80 percent, each bacterium divides with probability p. Now what's the probability that a single bacterium will lead to an everlasting colony? https://fivethirtyeight.com/features/how-long-will-the-bacterial-colony-last/ Strategy We will begin with the general scenario where the probability of each bacterium dividing is $p$. Let $P_n$ represent the probability that a single R. classicum bacterium, will lead to an everlasting colony. For this to happen, the first generation needs to succeed. This means that at least one of the first two children would definitely need to survive. The probability that both the two children do not survive is given by $(1-P_n)&#94;2$. $$ \\begin{align*} P_n &= p (1- (1-P_n)&#94;2)\\\\ P_n &= pP_n(2+P_n) \\end{align*} $$ The solutions of the above equation are $P_n = 0$ or $P_n = 2- \\frac{1}{p}$. For $P_n = 2 - \\frac{1}{p}$ to be a valid probability, $P_n \\geq 0 \\implies p \\geq \\frac{1}{2}$. Thus, $$ \\begin{align*} P_n &= \\begin{cases} 2-\\frac{1}{p} & p \\geq \\frac{1}{2} \\\\ 0 & p < \\frac{1}{2} \\end{cases} \\end{align*} $$ Essentially, if the death probability is at least $0.5$, then it is improbably for the colony to keep growing. For $p=0.8$, $P_n = 0.75$.","tags":"misc","url":"https://saket-choudhary.me/fivethirtyeight-riddler-a-curious-case-of-colonies.html","loc":"https://saket-choudhary.me/fivethirtyeight-riddler-a-curious-case-of-colonies.html"},{"title":"Fourier transform and choice of window function","text":"In [13]: % pylab inline import matplotlib as mpl from scipy import signal from scipy.signal import get_window import ipywidgets as widgets from IPython.display import display from ipywidgets import interact , interactive , fixed , interact_manual from matplotlib.ticker import ScalarFormatter , AutoMinorLocator mpl . rcParams [ 'grid.color' ] = 'k' mpl . rcParams [ 'grid.linestyle' ] = ':' mpl . rcParams [ 'grid.linewidth' ] = 0.5 mpl . rcParams [ 'font.size' ] = 32 mpl . rcParams [ 'figure.autolayout' ] = True mpl . rcParams [ 'figure.figsize' ] = ( 7.2 , 4.45 ) mpl . rcParams [ 'axes.titlesize' ] = 32 mpl . rcParams [ 'axes.labelsize' ] = 32 mpl . rcParams [ 'lines.linewidth' ] = 2 mpl . rcParams [ 'lines.markersize' ] = 6 mpl . rcParams [ 'legend.fontsize' ] = 13 mpl . rcParams [ 'mathtext.fontset' ] = 'stix' mpl . rcParams [ 'font.family' ] = 'STIXGeneral' mpl . rcParams [ 'lines.linewidth' ] = 3.5 mpl . rcParams [ 'xtick.labelsize' ] = 32 mpl . rcParams [ 'ytick.labelsize' ] = 32 mpl . rcParams [ 'legend.fontsize' ] = 32 AVAILABLE_WINDOWS = [ 'boxcar' , 'triang' , 'blackman' , 'hamming' , 'hann' , 'bartlett' , 'flattop' , 'parzen' , 'bohman' , 'blackmanharris' , 'nuttall' , 'barthann' ] COLORS = [ '#a6cee3' , '#1f78b4' , '#b2df8a' , '#33a02c' , '#fb9a99' , '#e31a1c' , '#fdbf6f' , '#ff7f00' , '#cab2d6' , '#6a3d9a' , '#ffff99' , '#b15928' ] def setup_axis ( ax ): ax . set_xlabel ( '' ) ax . yaxis . set_major_formatter ( ScalarFormatter ()) ax . yaxis . major . formatter . _useMathText = True ax . yaxis . set_minor_locator ( AutoMinorLocator ( 5 )) ax . xaxis . set_minor_locator ( AutoMinorLocator ( 5 )) ax . tick_params ( direction = 'out' , length = 12 , width = 2 , grid_alpha = 0.5 ) ax . tick_params ( direction = 'out' , which = 'minor' , length = 6 , width = 1 , grid_alpha = 0.5 ) ax . grid ( True ) def plot_windows ( windows , Nx = 4096 ): # boxcar, triang, blackman, hamming, hann, bartlett, # flattop, parzen, bohman, blackmanharris, nuttall, # barthann, # kaiser (needs beta), # gaussian (needs standard deviation), # general_gaussian (needs power, width), # slepian (needs width), # dpss (needs normalized half-bandwidth), # chebwin (needs attenuation), # exponential (needs decay scale), # tukey (needs taper fraction) if isinstance ( windows , unicode ) or isinstance ( windows , str ): windows = [ windows ] fig , ax = plt . subplots ( figsize = ( 15 , 12 )) for color , window in zip ( COLORS , windows ): gwindow = get_window ( window , Nx = Nx ) ax . plot ( gwindow , label = window , color = color , linestyle = '-.' ) #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5)) ax . legend ( bbox_to_anchor = ( 0. , 1.02 , 1. , . 102 ), loc = 3 , ncol = 3 , mode = \"expand\" , borderaxespad = 0. ) setup_axis ( ax ) ax . set_xlabel ( 'Time (s)' ) ax . set_ylabel ( 'Amplitude' ) fig . tight_layout () return fig select = widgets . SelectMultiple ( options = AVAILABLE_WINDOWS , value = ( 'flattop' ,), disabled = False , ) #interact(plot_windows, windows=select, Nx=(1, 4097)) fig = plot_windows ( windows = [ 'boxcar' ], Nx = 100 ) fig . savefig ( './plots/window_boxcar.svg' ) Populating the interactive namespace from numpy and matplotlib /home/saket/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['select', 'interactive'] `%matplotlib` prevents importing * from pylab and numpy \"\\n`%matplotlib` prevents importing * from pylab and numpy\" /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [81]: fig = plot_windows ( windows = [ 'boxcar' , 'hann' ], Nx = 100 ) fig . savefig ( './plots/window_boxcar_hanning.svg' ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [80]: fig = plot_windows ( windows = [ 'boxcar' , 'hann' , 'flattop' ], Nx = 100 ) fig . savefig ( './plots/window_boxcar_hanning_flattop.svg' ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [85]: def plot_freq_mix ( freq1 , freq2 , windows ): if isinstance ( windows , unicode ) or isinstance ( windows , str ): windows = [ windows ] fig = plt . figure ( figsize = ( 16 , 12 )) ax = plt . subplot ( 211 ) t = np . arange ( 0 , 60 , step = 1 ) s = np . sin ( 2 * t * np . pi * freq1 ) + np . sin ( 2 * t * np . pi * freq2 ) setup_axis ( ax ) ax . plot ( t , s ) ax . set_xlabel ( 'Time (s)' ) ax . set_ylabel ( 'Amplitude' ) ax = plt . subplot ( 212 ) for color , window in zip ( COLORS , windows ): n = 1024 w = np . fft . rfft ( s * get_window ( window , 60 ), n = n ) freqs = np . fft . rfftfreq ( n , d = t [ 1 ] - t [ 0 ]) ax . plot ( freqs , 20 * np . log10 ( np . abs ( w )), label = window , color = color ) #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=3) ax . legend ( bbox_to_anchor = ( 0. , 1.02 , 1. , . 102 ), loc = 3 , ncol = 4 , mode = \"expand\" , borderaxespad = 0. ) setup_axis ( ax ) ax . set_xlabel ( 'Frequnecy (Hz)' ) ax . set_ylabel ( 'decibels' ) ax . axvline ( 1 / 3.0 , color = 'red' , linestyle = 'dashed' ) fig . tight_layout () return fig #interact(plot_freq_mix, # freq1=widgets.FloatSlider(min=0.1,max=30,step=0.01,value=0.33), # freq2=widgets.FloatSlider(min=0.1,max=30,step=0.01,value=0.33), # windows=select,) plot_freq_mix ( 0.33 , 0.4 , AVAILABLE_WINDOWS ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" Out[85]: In [86]: fig = plot_freq_mix ( 0.33 , 0.33 , AVAILABLE_WINDOWS ) fig . savefig ( './plots/compare_windows_over_sine_all.svg' ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [78]: fig = plot_freq_mix ( 0.33 , 0.33 , [ 'boxcar' , 'hann' , 'flattop' ]) fig . savefig ( './plots/compare_windows_over_sine.svg' ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [82]: /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [71]: def plot_ft ( freq1 = 0.33 , sampling_point = 10 , windows = 'hann' ): fig = plt . figure ( figsize = ( 25 , 20 )) ax = plt . subplot ( 211 ) if isinstance ( windows , unicode ) or isinstance ( windows , str ): windows = [ windows ] # 1Hz sampling t = np . arange ( 0 , 60 , step = 0.1 ) # freq1 Hz signal s = np . sin ( 2 * t * np . pi * freq1 ) setup_axis ( ax ) ax . plot ( t , s ) ax . set_xlabel ( 'Time (s)' ) ax . set_ylabel ( 'Amplitude' ) ax = plt . subplot ( 212 ) for color , window in zip ( COLORS , windows ): n = 1024 w = np . fft . rfft ( s [ 0 : sampling_point ] * get_window ( window , sampling_point ), n = n ) freqs = np . fft . rfftfreq ( n , d = 0.1 ) ax . plot ( freqs , 20 * np . log10 ( np . abs ( w )), label = window , color = color ) #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=3) ax . legend ( bbox_to_anchor = ( 0. , 1.02 , 1. , . 102 ), loc = 3 , ncol = 4 , mode = \"expand\" , borderaxespad = 0. ) setup_axis ( ax ) ax . set_xlabel ( 'Frequnecy (Hz)' ) ax . set_ylabel ( 'decibels' ) ax . axvline ( 3.0 , color = 'red' , linestyle = 'dashed' ) plot_ft () /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [72]: plot_ft ( sampling_point = 50 ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [73]: N = 2048 x = np . sin ( 2 * np . pi * 10 * np . linspace ( 0 , 10 , N )) + np . random . random ( N ) * 0.1 z = np . fft . rfft ( x ) # FFT y = np . fft . rfftfreq ( len ( x )) # Frequency data fig , ax = plt . subplots () ax . plot ( y , z ) /home/saket/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:492: ComplexWarning: Casting complex values to real discards the imaginary part return array(a, dtype, copy=False, order=order) Out[73]: [<matplotlib.lines.Line2D at 0x7f4991faa1d0>] /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [46]: import numpy as np import matplotlib.pyplot as plt fs = 10 T = 1.4 t = np . arange ( T * fs ) / fs freq = 2.6 y = np . cos ( 2 * np . pi * freq * t ) fy = np . fft . fft ( y ) magfy = np . abs ( fy ) freqs = np . fft . fftfreq ( len ( fy ), d = 1 / fs ) plt . plot ( freqs , magfy , 'd' , label = 'no padding' ) for ( factor , markersize ) in [( 2 , 9 ), ( 16 , 4 )]: fy_padded = np . fft . fft ( y , factor * len ( y )) magfy_padded = np . abs ( fy_padded ) freqs_padded = np . fft . fftfreq ( len ( fy_padded ), d = 1 / fs ) plt . plot ( freqs_padded , magfy_padded , '.' , label = 'padding factor %d ' % factor , alpha = 0.5 , markersize = markersize ) /home/saket/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(\"This figure includes Axes that are not compatible \" In [ ]:","tags":"misc","url":"https://saket-choudhary.me/fourier-transform-and-choice-of-window-function.html","loc":"https://saket-choudhary.me/fourier-transform-and-choice-of-window-function.html"},{"title":"Expected number of flips in a memoryless concentration game","text":"Riddler Classic https://fivethirtyeight.com/features/how-many-phones-do-you-need-to-win-hq-trivia/ From Kate Seely, a gaming problem to which she would very much like to know the answer: I have a matching game app for my 4-year-old daughter. There are 10 different pairs of cards, each pair depicting the same animal. That makes 20 cards total, all arrayed face down. The goal is to match all the pairs. When you flip two cards up, if they match, they stay up, decreasing the number of unmatched cards and rewarding you with the corresponding animal sound. If they don't match, they both flip back down. (Essentially like Concentration. )) However, my 1-year-old son also likes to play the game, exclusively for its animal sounds. He has no ability to match cards intentionally — it's all random. If he flips a pair of cards every second and it takes another second for them to either flip back over or to make the \"matching\" sound, how long should my daughter expect to have to wait before he finishes the game and it's her turn again? Solution Consider a more general case of $n$ pairs (hence a total of $2n$ cards). The probability of getting a \"hit\", i.e. a flip with matching pairs starting with $n$ pairs is given by: \\begin{align*} p_n &= \\frac{{n \\choose 1}}{2n \\choose 2} \\\\ &= \\frac{1}{2n-1} \\end{align*} The number of tries till a first \"hit\" follows a geometric disribution. (Think of a coin toss experiment where the we are studying the distribution of number of heads till the first tail appears $P(X=k) = p&#94;k(1-p)$. Then the mean is given by $EX = \\frac{1}{p}$. Thus, it will take $2n-1$ picks to get a \"hit\" on average starting with $2n$ cards. Similarly, it will take $2n-3$ picks starting with $2n-2$ cards and so on. Hence number of picks when the player is playing randomly: $$ (2n-1) + (2n-3) + \\dots + 3 + 1 = n&#94;2 $$ We verify this solution with simulation below where we simulate picks for $n=[10, 20, 30, 40, 50]$ In [1]: % pylab inline import seaborn as sns from collections import defaultdict sns . set_context ( 'paper' , font_scale = 2 ) sns . set_style ( 'whitegrid' ) Populating the interactive namespace from numpy and matplotlib In [2]: np . random . seed ( 42 ) iterations = 1000 N = np . arange ( 10 , 51 , 10 ) n_tries_arr = defaultdict ( list ) for n in N : print ( n ) for iteration in range ( 0 , iterations ): sample_universe = list ( range ( 1 , n + 1 )) + list ( range ( 1 , n + 1 )) n_tries = 0 while sample_universe : samples = np . random . choice ( sample_universe , size = 2 , replace = False ) sample1 , sample2 = samples if sample1 == sample2 : sample_universe = list ( filter ( lambda a : a != sample1 , sample_universe )) n_tries += 1 n_tries_arr [ n ] . append ( n_tries ) 10 20 30 40 50 In [3]: fig , ax = plt . subplots ( figsize = ( 8 , 8 )) ax . plot ([ np . mean ( n_tries_arr [ x ]) for x in N ], np . array ( N ) ** 2 ) ax . set_xlabel ( '$N&#94;2$' ) ax . set_ylabel ( 'Expected Runtime' ) fig . tight_layout ()","tags":"riddles","url":"https://saket-choudhary.me/expected-number-of-flips-in-a-memoryless-concentration-game.html","loc":"https://saket-choudhary.me/expected-number-of-flips-in-a-memoryless-concentration-game.html"},{"title":"Andrews Curves","text":"In [1]: % pylab inline import pandas as pd import matplotlib.patches as mpatches from sklearn.decomposition import PCA plt . style . use ( 'seaborn-colorblind' ) CB_color_cycle = [ '#377eb8' , '#ff7f00' , '#4daf4a' , '#f781bf' , '#a65628' , '#984ea3' , '#999999' , '#e41a1c' , '#dede00' ] Populating the interactive namespace from numpy and matplotlib Andrews Curves D. F. Andrews introduced 'Andrews Curves' in his 1972 paper for plotthing high dimensional data in two dimeion. The underlying principle is simple: Embed the high dimensiona data in high diemnsion only using a space of functions and then visualizing these functions. Consider A $d$ dimensional data point $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$. Define the following function: $$f_x(t) = \\begin{cases} \\frac{x_1}{\\sqrt{2}} + x_2 \\sin(t) + x_3 \\cos(t) + x_4 \\sin (2t) + x_5\\cos(2t) + \\dots + x_{2k} \\sin(kt) + x_{2k+1} \\cos(kt) + \\dots + x_{d-2}\\sin( (\\frac{d}{2} -1)t) + x_{d-1}\\cos( (\\frac{d}{2} -1)t) + x_{d} \\sin(\\frac{d}{2}t) & d \\text{ even}\\\\ \\frac{x_1}{\\sqrt{2}} + x_2 \\sin(t) + x_3 \\cos(t) + x_4 \\sin (2t) + x_5\\cos(2t) + \\dots + x_{2k} \\sin(kt) + x_{2k+1} \\cos(kt) + \\dots + x_{d-3}\\sin( \\frac{d-3}{2} t) + x_{d-2}\\cos( \\frac{d-3}{2}t) + x_{d-1} \\sin(\\frac{d-1}{2}t) + x_{d} \\cos(\\frac{d-1}{2}t)) & d \\text{ odd}\\\\ \\end{cases} $$ This representation yields one dimensional projections, which may reveal clustering, outliers or orther patterns that occur in this subspace. All such one dimensional projections can then be plotted on one graph. Properties Andrews Curves has some intersting properties that makes it useful as a 2D tool: Mean If $\\bar{\\mathbf{x}}$ represents the mean of $\\bar{x}$ for $n$ observations: $\\bar{\\mathbf{x}} = \\frac{1}{n} \\mathbf{x_i}$. then, $$ f_{\\bar{\\mathbf{x}}}(t) = \\frac{1}{n} \\sum_{i=1}{n} f_{\\mathbf{x_i}}(t)$$ Proof: We consider an odd $d$. \\begin{align*} f_{\\bar{\\mathbf{x}}}(t) &= \\frac{\\bar{\\mathbf{x_1}}}{\\sqrt{2}} + \\bar{\\mathbf{x_2}} \\sin(t) + \\bar{\\mathbf{x_3}} \\cos(t) + \\bar{\\mathbf{x_4}} \\sin(2t) + \\bar{\\mathbf{x_5}} \\cos(2t) + \\dots + \\bar{\\mathbf{x_d}} \\sin(\\frac{d}{2}t) \\\\ &= \\frac{\\sum_{j=1}&#94;n x_{1j}}{\\sqrt{2}} + \\frac{\\sum_{j=1}x_{2j}}{n} \\sin(t) + \\frac{\\sum_{j=1}x_{3j}}{n} \\cos(t) + \\frac{\\sum_{j=1}x_{4j}}{n}\\sin(2t) + \\frac{\\sum_{j=1}x_{5j}}{n}\\cos(2t) + \\dots + \\frac{\\sum_{j=1}x_{dj}}{n} \\sin(\\frac{d}{2}t)\\\\ &= \\frac{1}{n} \\sum_{i=1}&#94;n f_{x_i} (t) \\end{align*} Distance Euclidean distance is preserved. Consider two points $\\mathbf{x}$ and $\\mathbf{y}$ $$||\\mathbf{x} - \\mathbf{y}||_2&#94;2 = \\sum_{j=1}&#94;d |x_j-y_j|&#94;2$$ Let's consider $||f_{\\mathbf{x}}(t) - f_{\\mathbf{y}}(t) ||_2&#94;2 = \\int_{-\\pi}&#94;{\\pi} (f_{\\mathbf{x}}(t) - f_{\\mathbf{y}}(t))&#94;2 dt $ \\begin{align*} \\int_{-\\pi}&#94;{\\pi} (f_{\\mathbf{x}}(t) - f_{\\mathbf{y}}(t))&#94;2 dt &= \\frac{(x_1-y_1)&#94;2}{2}(2\\pi) + \\int_{-\\pi}&#94;{\\pi} (x_1-y_1)&#94;2 \\sin&#94;2{t}\\ dt + \\int_{-\\pi}&#94;{\\pi} (x_2-y_2)&#94;2 \\cos&#94;2{t}\\ dt + \\int_{-\\pi}&#94;{\\pi} (x_3-y_3)&#94;2 \\sin&#94;2{2t}\\ dt + \\int_{-\\pi}&#94;{\\pi} (x_4-y_4)&#94;2 \\cos&#94;2{2t}\\ dt + \\dots \\end{align*}\\begin{align*} \\int&#94;{\\pi}_{-\\pi} \\sin&#94;2 (kt) dt &= \\frac{1}{k}\\int_{-k\\pi}&#94;{k\\pi} \\sin&#94;2 (t') dt'\\\\ &= \\frac{1}{k} \\left( \\frac{\\int_{-k\\pi}&#94;{k\\pi} (1-\\cos{(2t'))}dt'}{2} \\right)\\\\ &= \\frac{1}{k} \\frac{2k\\pi}{2}\\\\ &= \\pi\\\\ \\int&#94;{\\pi}_{-\\pi} \\cos&#94;2 (kt) dt &= \\int&#94;{\\pi}_{-\\pi} (1-\\sin&#94;2 (kt)) dt\\\\ &= 2\\pi-\\pi\\\\ &= \\pi \\end{align*} Thus, \\begin{align*} \\int_{-\\pi}&#94;{\\pi} (f_{\\mathbf{x}}(t) - f_{\\mathbf{y}}(t))&#94;2 dt &= \\pi ||\\mathbf{x} - \\mathbf{y}||_2&#94;2 \\end{align*} Variance If the $d$ features/components are all indepdent and have a common variance $\\sigma&#94;2$ Then \\begin{align*} \\text{Var}f_{\\mathbf{x}(t)} &= \\text{Var} \\left(\\frac{x_1}{\\sqrt{2}} + x_2 \\sin(t) + x_3 \\cos(t) + x_4 \\sin (2t) + x_5\\cos(2t) + \\dots + x_{2k} \\sin(kt) + x_{2k+1} \\cos(kt) + \\dots + x_{d-2}\\sin( (\\frac{d}{2} -1)t) + x_{d-1}\\cos( (\\frac{d}{2} -1)t) + x_{d} \\sin(\\frac{d}{2}t) \\right)\\\\ &= \\sigma&#94;2 \\left( \\frac{1}{2} + \\sin&#94;2 + \\cos&#94;2 t + \\sin&#94;2 2t + \\cos&#94;2 2t + \\dots \\right)\\\\ &= \\begin{cases} \\sigma&#94;2(\\frac{1}{2} + \\frac{k-1}{2}) & d \\text{ odd }\\\\ \\sigma&#94;2(\\frac{1}{2} + \\frac{k}{2} - 1 + \\sin&#94;2 {\\frac{kt}{2}} ) & d \\text{ even }\\\\ \\end{cases}\\\\ &= \\begin{cases} \\frac{k\\sigma&#94;2}{2} & d \\text{ odd }\\\\ \\sigma&#94;2(\\frac{k-1}{2} + \\sin&#94;2 {\\frac{kt}{2}} ) & d \\text{ even }\\\\ \\end{cases} \\end{align*} In the even case the variance is boundded between $[\\sigma&#94;2(\\frac{k-1}{2}), \\sigma&#94;2(\\frac{k+1}{2})]$ Since the variance is indepedent of $t$, the plotted functions will be smooth! Interpretation Clustering Functions close together, forming a band imply the corresponding points are also close in the euclidean space Test of significance at particular values of $t$ To test $f_{\\mathbf{x}}(t) = f_{\\mathbf{y}}(t)$ for some hypothesize $\\mathbf{y}$ and assuming the $\\text{Var}[f_{\\mathbf{x}}(t)]$ is known then testing can be done using the usual $z$ score: $$ z = \\frac{f_{\\mathbf{x}}(t)-f_{\\mathbf{y}}(t)}{(\\text{Var}[{f_{\\mathbf{x}}(t)}])&#94;{\\frac{1}{2}}} $$ assuming that the comoponets $x_i$ are independent normal random variables. Detecting outliers If comonents $x_i$ are independent normal $ x_i \\sim \\mathcal{N}(\\mu_i, \\sigma&#94;2)$, then $\\frac{|\\mathbf{x}-\\mathbf{\\mu}}{\\sigma&#94;2}$ follows a $\\chi&#94;2_d$ distirbution. Consider a vector $v = \\frac{f_\\mathbf{1}(t)}{||f_\\mathbf{1}(t)||}$ then : \\begin{align*} |(\\mathbf{x}-\\mathbf{\\mu})'v|&#94;2 &= \\frac{||f_{\\mathbf{x}}(t) - f_{\\mathbf{\\mu}}(t)||&#94;2 }{||f_\\mathbf{1}(t)||&#94;2} \\frac{||f_{\\mathbf{x}}(t) - f_{\\mathbf{\\mu}}(t)||&#94;2 }{||f_\\mathbf{1}(t)||&#94;2} &\\leq \\chi_d&#94;2(\\alpha) \\end{align*} Now, \\begin{align*} ||f_\\mathbf{1}(t)||&#94;2 &= \\frac{1}{2} + \\sin&#94;2 + \\cos&#94;2 t + \\dots + \\\\ &\\leq \\frac{d+1}{2} \\end{align*} Thus, \\begin{align*} ||f_{\\mathbf{x}}(t) - f_{\\mathbf{\\mu}}(t)||&#94;2 \\leq \\sigma&#94;2 ||f_\\mathbf{1}(t)||&#94;2 \\chi&#94;2_d(\\alpha) &\\leq \\sigma&#94;2 \\frac{d+1}{2} \\chi&#94;2_d(\\alpha)\\\\ \\end{align*} Linear relationships The \"Sandwich\" theorem: If $\\mathbf{y}$ lies on a line joining $\\mathbf{x}$ and $\\mathbf{z}$, then $\\forall t$ : $f_\\mathbf{y}(t)$ lies between $f_\\mathbf{x}(t)$ and $f_\\mathbf{z}(t)$. This is straightforward. In [2]: def andrews_curves ( data , granularity = 1000 ): \"\"\" Parameters ----------- data : array like ith row is the ith observation jth column is the jth feature Size (m, n) => m replicats with n features granularity : int linspace granularity for theta Returns ------- matrix : array Size (m, granularity) => \"\"\" n_obs , n_features = data . shape theta = np . linspace ( - np . pi , np . pi , granularity ) # transpose theta = np . reshape ( theta , ( - 1 , theta . shape [ 0 ])) t = np . arange ( 1 , np . floor ( n_features / 2 ) + 1 ) t = np . reshape ( t , ( t . shape [ 0 ], 1 )) sin_bases = np . sin ( t * theta ) cos_bases = np . cos ( t * theta ) if n_features % 2 == 0 : # Remove the last row of cosine bases # for even values cos_bases = cos_bases [: - 1 ,:] c = np . empty (( sin_bases . shape [ 0 ] + cos_bases . shape [ 0 ], sin_bases . shape [ 1 ] ), dtype = sin_bases . dtype ) c [ 0 :: 2 ,:] = sin_bases c [ 1 :: 2 ,:] = cos_bases constant = 1 / np . sqrt ( 2 ) * np . ones (( 1 , c . shape [ 1 ])) matrix = np . vstack ([ constant , c ]) return ( np . dot ( data , matrix )) Andrews Curves for iris dataset In [3]: df = pd . read_csv ( 'https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/data/iris.csv' ) df_grouped = df . groupby ( 'Name' ) In [4]: df_setosa = df . query ( \"Name=='Iris-setosa'\" ) fig , ax = plt . subplots ( figsize = ( 8 , 8 )) index = 0 patches = [] for key , group in df_grouped : group = group . drop ( 'Name' , axis = 1 ) for row in andrews_curves ( group . as_matrix ()): plot = ax . plot ( row , CB_color_cycle [ index ]) patch = mpatches . Patch ( color = CB_color_cycle [ index ], label = key ) index += 1 patches . append ( patch ) ax . legend ( handles = patches ) fig . tight_layout () PCA In [5]: X = df [[ 'SepalLength' , 'SepalWidth' , 'PetalLength' , 'PetalWidth' ]] y = df [ 'Name' ] . astype ( 'category' ) . cat . codes target_names = df [ 'Name' ] . astype ( 'category' ) . unique () pca = PCA ( n_components = 2 ) X_r = pca . fit ( X ) . transform ( X ) fig , ax = plt . subplots ( figsize = ( 8 , 8 )) colors = CB_color_cycle [: 3 ] lw = 2 for color , i , target_name in zip ( colors , [ 0 , 1 , 2 ], target_names ): plt . scatter ( X_r [ y == i , 0 ], X_r [ y == i , 1 ], color = color , alpha =. 8 , lw = lw , label = target_name ) ax . legend ( loc = 'best' , shadow = False , scatterpoints = 1 ) ax . set_xlabel ( 'Variance explained: {:.2f} ' . format ( pca . explained_variance_ratio_ [ 0 ])) ax . set_ylabel ( 'Variance explained: {:.2f} ' . format ( pca . explained_variance_ratio_ [ 1 ])) ax . set_title ( 'PCA of IRIS dataset' ) fig . tight_layout () Clearly setos and virginica lie close to each other and hence appear as merged clusters in PCA and merged bands in Andrews Curves In [ ]:","tags":"misc","url":"https://saket-choudhary.me/andrews-curves.html","loc":"https://saket-choudhary.me/andrews-curves.html"},{"title":"#AcademicValentines","text":"In [1]: # Credits: http://www.walkingrandomly.com/?p=5964 import warnings warnings . filterwarnings ( 'ignore' ) import matplotlib.pyplot as plt import numpy as np import seaborn as sns sns . set_style ( 'white' ) from matplotlib.animation import FuncAnimation from IPython.display import HTML fig = plt . figure () ax = plt . axes ( xlim = ( - 2 , 2 ), ylim = ( - 2 , 2 )) line , = ax . plot ([], [], lw = 2 , color = 'red' ) time_text1 = ax . text ( - 0.5 , 1.5 , \"I've read\" , fontsize = 18 , color = 'black' ) time_text2 = ax . text ( - 0.5 , - 1.7 , \"your work!\" , fontsize = 18 , color = 'black' ) def init (): line . set_data ([], []) return line , def animate ( i ): x = np . linspace ( - 2 , 2 , 1000 ) y = ( np . sqrt ( np . cos ( x )) * np . cos ( i * x ) \\ + np . sqrt ( np . abs ( x )) - 0.7 ) * ( 4 - x * x ) ** 0.01 line . set_data ( x , y ) time_text1 . set_position ([ - np . sin ( i / 8.0 ) - 0.25 , 1.5 ]) time_text2 . set_position ([ - np . sin ( i / 8.0 ) - 0.25 , - 1.7 ]) return line , sns . despine () animation = FuncAnimation ( fig , animate , init_func = init , frames = 120 , interval = 25 ) HTML ( animation . to_html5_video ()) Out[1]: Your browser does not support the video tag.","tags":"misc","url":"https://saket-choudhary.me/academicvalentines.html","loc":"https://saket-choudhary.me/academicvalentines.html"},{"title":"Animated Heart in Python","text":"In [2]: # Credits: http://www.walkingrandomly.com/?p=5964 # and http://jakevdp.github.io/blog/2013/05/19/a-javascript-viewer-for-matplotlib-animations/ import warnings warnings . filterwarnings ( 'ignore' ) import matplotlib.pyplot as plt from matplotlib import animation import numpy as np from JSAnimation import IPython_display fig = plt . figure () ax = plt . axes ( xlim = ( - 2 , 2 ), ylim = ( - 2 , 2 )) line , = ax . plot ([], [], lw = 2 , color = 'red' ) def init (): line . set_data ([], []) return line , def animate ( i ): x = np . linspace ( - 2 , 2 , 1000 ) y = ( np . sqrt ( np . cos ( x )) * np . cos ( i * x ) + np . sqrt ( np . abs ( x )) - 0.7 ) * ( 4 - x * x ) ** 0.01 line . set_data ( x , y ) return line , animation . FuncAnimation ( fig , animate , init_func = init , frames = 100 , interval = 30 ) Out[2]: – + Once Loop Reflect","tags":"misc","url":"https://saket-choudhary.me/animated-heart-in-python.html","loc":"https://saket-choudhary.me/animated-heart-in-python.html"},{"title":"Expectation Maximisation in Python: Coin Toss Example","text":"Expectation Maximisation with Python : Coin Toss This notebook implements the example, I consider a classic for understanding Expectation Maximisation. See: http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html Notations: \\begin{align*} \\theta_A &= \\text{Probability of a Heads showing up given the coin tossed is A}\\\\ \\theta_B &= \\text{Probability of a Heads showing up given the coin tossed is B}\\\\ \\end{align*} In [3]: % matplotlib notebook from __future__ import division from collections import OrderedDict from scipy.stats import binom as binomial import numpy as np import matplotlib.pyplot as plt import seaborn as sns #from ipywidgets import StaticInteract, RangeWidget import pandas as pd from IPython.display import display , Image from scipy.spatial.distance import euclidean from sympy import init_printing , symbols , Eq init_printing () In [4]: Image ( 'images/nbt1406-F1.png' ) Out[4]: In [5]: coin_toss = [] coin_toss . append ( 'H T T T H H T H T H' . split ()) coin_toss . append ( 'H H H H T H H H H H' . split ()) coin_toss . append ( 'H T H H H H H T H H' . split ()) coin_toss . append ( 'H T H T T T H H T T' . split ()) coin_toss . append ( 'T H H H T H H H T H' . split ()) In [7]: columns = range ( 1 , 11 ) df = pd . DataFrame ( coin_toss , index = None , columns = columns ) df . index . rename ( 'Toss' ) Out[7]: Int64Index([0, 1, 2, 3, 4], dtype='int64', name=u'Toss') Our configuration looks like this: In [8]: df Out[8]: 1 2 3 4 5 6 7 8 9 10 0 H T T T H H T H T H 1 H H H H T H H H H H 2 H T H H H H H T H H 3 H T H T T T H H T T 4 T H H H T H H H T H Case 1: Identity of coin being tossed known If the identity of the coin being tossed is known and is observed = ['B', 'A', 'A', 'B', 'A'] it is not so difficult to calculate the corresponding values of $\\theta_A$ and $\\theta_B$: $$ \\theta_A = \\frac{\\text{Total Heads when coin tossed is A}}{\\text{Total tosses for coin A}} $$$$ \\theta_B = \\frac{\\text{Total Heads when coin tossed is B}}{\\text{Total tosses for coin B}} $$ In [9]: thetaA , thetaB = symbols ( 'theta_A theta_B' ) a , b = thetaA , thetaB # Hack to display In [10]: ## Observed Case observed = [ 'B' , 'A' , 'A' , 'B' , 'A' ] index_A = [ i for i , x in enumerate ( observed ) if x == 'A' ] index_B = [ i for i , x in enumerate ( observed ) if x == 'B' ] total_tosses = df . size A_tosses = df . iloc [ index_A ] . unstack () B_tosses = df . iloc [ index_B ] . unstack () A_heads = A_tosses . value_counts ()[ 'H' ] B_heads = B_tosses . value_counts ()[ 'H' ] theta_A = A_heads / A_tosses . size theta_B = B_heads / B_tosses . size In [11]: ( a , theta_A ) Out[11]: $$\\left ( \\theta_{A}, \\quad 0.8\\right )$$ In [12]: ( b , theta_B ) Out[12]: $$\\left ( \\theta_{B}, \\quad 0.45\\right )$$ Case 2 Identity of coin being tossed is unknown When the identity of coin being tossed is unknwon we rely on Expectation Maximisation to give us the estimates of $\\theta_A$ and $\\theta_B$. We start with an initial value of $\\theta_A, \\theta_B$ and then given the observed data (the 50 coin tosses) run the 'E-step' calculating the probability of coin $A$ or $B$ being used for a series of tosses(Remember each set of 10 coin tosses is done using In [14]: thetaA = 0.6 thetaB = 0.5 def em ( theta_old ): row_prob = [] ## Expectation for row in coin_toss : count_heads = row . count ( 'H' ) p_a = binomial . pmf ( count_heads , len ( row ), theta_old [ 'A' ]) p_b = binomial . pmf ( count_heads , len ( row ), theta_old [ 'B' ]) p_t = p_a + p_b p_a = p_a / p_t p_b = p_b / p_t row_prob . append ({ 'A' : p_a , 'B' : p_b , 'count_heads' : count_heads , 'total_tosses' : len ( row )}) ## Maximisation new_coin_toss = [] for row in row_prob : total_tosses = row [ 'total_tosses' ] total_heads = row [ 'count_heads' ] A_heads = row [ 'A' ] * total_heads A_tails = row [ 'A' ] * ( total_tosses - total_heads ) B_heads = row [ 'B' ] * total_heads B_tails = row [ 'B' ] * ( total_tosses - total_heads ) new_coin_toss . append ([ A_heads , A_tails , B_heads , B_tails ]) df = pd . DataFrame ( new_coin_toss , columns = [ 'A Heads' , 'A Tails' , 'B Heads' , 'B Tails' ]) new_pa = df [ 'A Heads' ] . sum () / ( df [ 'A Heads' ] . sum () + df [ 'A Tails' ] . sum ()) new_pb = df [ 'B Heads' ] . sum () / ( df [ 'B Heads' ] . sum () + df [ 'B Tails' ] . sum ()) new_theta = OrderedDict ({ 'A' : new_pa , 'B' : new_pb }) display ( df ) return new_theta theta = OrderedDict ({ 'A' : thetaA , 'B' : thetaB }) In [15]: theta_new = OrderedDict () max_iterations = 10000 iterations = 0 diff = 1 tolerance = 1e-6 while ( iterations < max_iterations ) and ( diff > tolerance ): new_theta = em ( theta ) diff = euclidean ( new_theta . values (), theta . values ()) theta = new_theta A Heads A Tails B Heads B Tails 0 2.245745 2.245745 2.754255 2.754255 1 7.244870 0.804986 1.755130 0.195014 2 5.867737 1.466934 2.132263 0.533066 3 1.408625 2.112937 2.591375 3.887063 4 4.530506 1.941645 2.469494 1.058355 A Heads A Tails B Heads B Tails 0 1.479097 1.479097 3.520903 3.520903 1 7.303594 0.811510 1.696406 0.188490 2 5.651376 1.412844 2.348624 0.587156 3 0.760578 1.140867 3.239422 4.859133 4 4.014738 1.720602 2.985262 1.279398 A Heads A Tails B Heads B Tails 0 1.087962 1.087962 3.912038 3.912038 1 7.828637 0.869849 1.171363 0.130151 2 6.009233 1.502308 1.990767 0.497692 3 0.446362 0.669544 3.553638 5.330456 4 4.038084 1.730607 2.961916 1.269393 A Heads A Tails B Heads B Tails 0 0.808513 0.808513 4.191487 4.191487 1 8.216144 0.912905 0.783856 0.087095 2 6.354109 1.588527 1.645891 0.411473 3 0.265334 0.398001 3.734666 5.601999 4 4.109732 1.761314 2.890268 1.238686 A Heads A Tails B Heads B Tails 0 0.645102 0.645102 4.354898 4.354898 1 8.418405 0.935378 0.581595 0.064622 2 6.572406 1.643101 1.427594 0.356899 3 0.179981 0.269971 3.820019 5.730029 4 4.159435 1.782615 2.840565 1.217385 A Heads A Tails B Heads B Tails 0 0.567711 0.567711 4.432289 4.432289 1 8.507517 0.945280 0.492483 0.054720 2 6.681854 1.670464 1.318146 0.329536 3 0.144896 0.217344 3.855104 5.782656 4 4.185923 1.793967 2.814077 1.206033 A Heads A Tails B Heads B Tails 0 0.535440 0.535440 4.464560 4.464560 1 8.544022 0.949336 0.455978 0.050664 2 6.730149 1.682537 1.269851 0.317463 3 0.131238 0.196856 3.868762 5.803144 4 4.198972 1.799559 2.801028 1.200441 A Heads A Tails B Heads B Tails 0 0.522786 0.522786 4.477214 4.477214 1 8.558587 0.950954 0.441413 0.049046 2 6.750249 1.687562 1.249751 0.312438 3 0.126013 0.189019 3.873987 5.810981 4 4.205202 1.802230 2.794798 1.197770 A Heads A Tails B Heads B Tails 0 0.517957 0.517957 4.482043 4.482043 1 8.564351 0.951595 0.435649 0.048405 2 6.758425 1.689606 1.241575 0.310394 3 0.124026 0.186039 3.875974 5.813961 4 4.208106 1.803474 2.791894 1.196526 A Heads A Tails B Heads B Tails 0 0.516135 0.516135 4.483865 4.483865 1 8.566629 0.951848 0.433371 0.048152 2 6.761725 1.690431 1.238275 0.309569 3 0.123272 0.184909 3.876728 5.815091 4 4.209430 1.804042 2.790570 1.195958 A Heads A Tails B Heads B Tails 0 0.515451 0.515451 4.484549 4.484549 1 8.567530 0.951948 0.432470 0.048052 2 6.763053 1.690763 1.236947 0.309237 3 0.122987 0.184480 3.877013 5.815520 4 4.210025 1.804296 2.789975 1.195704 A Heads A Tails B Heads B Tails 0 0.515196 0.515196 4.484804 4.484804 1 8.567887 0.951987 0.432113 0.048013 2 6.763588 1.690897 1.236412 0.309103 3 0.122879 0.184318 3.877121 5.815682 4 4.210288 1.804409 2.789712 1.195591 A Heads A Tails B Heads B Tails 0 0.515100 0.515100 4.484900 4.484900 1 8.568028 0.952003 0.431972 0.047997 2 6.763804 1.690951 1.236196 0.309049 3 0.122838 0.184256 3.877162 5.815744 4 4.210403 1.804458 2.789597 1.195542 A Heads A Tails B Heads B Tails 0 0.515065 0.515065 4.484935 4.484935 1 8.568084 0.952009 0.431916 0.047991 2 6.763891 1.690973 1.236109 0.309027 3 0.122822 0.184233 3.877178 5.815767 4 4.210453 1.804480 2.789547 1.195520 A Heads A Tails B Heads B Tails 0 0.515051 0.515051 4.484949 4.484949 1 8.568106 0.952012 0.431894 0.047988 2 6.763926 1.690982 1.236074 0.309018 3 0.122816 0.184224 3.877184 5.815776 4 4.210474 1.804489 2.789526 1.195511 A Heads A Tails B Heads B Tails 0 0.515046 0.515046 4.484954 4.484954 1 8.568115 0.952013 0.431885 0.047987 2 6.763940 1.690985 1.236060 0.309015 3 0.122814 0.184221 3.877186 5.815779 4 4.210483 1.804493 2.789517 1.195507 In [59]: ( a , new_theta [ 'A' ]) Out[59]: $$\\left ( \\theta_{A}, \\quad 0.796788954444\\right )$$ In [60]: ( b , new_theta [ 'B' ]) Out[60]: $$\\left ( \\theta_{B}, \\quad 0.51958345063\\right )$$ In [ ]:","tags":"misc","url":"https://saket-choudhary.me/expectation-maximisation-in-python-coin-toss-example.html","loc":"https://saket-choudhary.me/expectation-maximisation-in-python-coin-toss-example.html"},{"title":"Entropy and Uniform Distribution","text":"To show \\(\\sum p_i\\log(p_i) = p\\) Consider \\(H = -\\sum p_i log(p_i) = log(n)\\) Consider weighted AM-GM for \\(p_i\\) : $$ (\\prod \\frac{1}{p_i}&#94;p_i)&#94;\\frac{1}{\\sum p_i} \\leq \\sum \\frac{1}{p_i} \\times p_i = n $$ Take log both sides: $$ \\sum p_i \\log(\\frac{1}{p_i}) \\leq \\log(n) $$ And hence euqality holds only if \\(p_i=p_j=1/n\\) $$ p_i = 1/n $$","tags":"misc","url":"https://saket-choudhary.me/entropy-and-uniform-distribution.html","loc":"https://saket-choudhary.me/entropy-and-uniform-distribution.html"},{"title":"A frequent inequality","text":"$$\\frac{x-1}{x} \\leq \\ln(x) \\leq x-1 \\forall\\ x>0$$ Consider \\(f(x)=\\ln(x)-\\frac{x-1}{x}\\) \\(f'(x) = \\frac{1}{x} - \\frac{1}{x&#94;2} = \\frac{x-1}{x&#94;2}\\) Now consider the following two cases: Case A: \\(0 < x \\leq 1\\) and Case B: \\(1 < x < \\infty\\) Then for Case A: \\(x-1 \\leq 0\\) and hence \\(f'(x)\\leq 0\\) \\(\\implies\\) \\(f(x)\\) is a non-increasing function in \\((0,1]\\) and hence \\(f(x)\\geq f(1) \\forall x \\in (0,1]\\) So for \\(x \\in (0,1]\\) , \\(f(x) \\geq f(1)\\) \\(\\implies\\) \\(\\ln(x)-\\frac{x-1}{x} \\geq 0\\) \\(\\implies\\) \\(\\frac{x-1}{x} \\leq \\ln(x)\\) Now for Case B: \\(1 < x < \\infty\\) \\(\\implies\\) \\(x-1>0\\) Thus, in this region \\(f'(x)>0\\) and hence \\(f(x)\\) is a strictly increasing function in \\((1,\\infty)\\) Thus, for \\(1 < x < \\infty\\) , \\(f(x)>f(1)\\) and hence \\(\\ln(x)-\\frac{x-1}{x} > 0\\) Combine results with that in case A for \\(f(x)\\) to conclude: \\(\\frac{x-1}{x} \\leq \\ln(x)\\) Now define \\(f(x)= \\ln(x)-x+1\\) \\(\\implies\\) \\(f'(x) = \\frac{1}{x}-1 =\\frac{1-x}{x}\\) Whole story of Case A and Case B again. But it should be simple to see that for \\(0 < x \\leq 1\\) , \\(g'(x)\\geq 0\\) and hence \\(g(x) \\leq g(1)\\) \\(\\implies\\) \\(\\ln(x)-x+1 \\leq 0\\) And for \\(1 < x < \\infty\\) , \\(g'(x)<0\\) and hence \\(g(x)<g(1)\\) \\(\\implies\\) \\(\\ln(x)-x+1 < 0\\)","tags":"misc","url":"https://saket-choudhary.me/a-frequent-inequality.html","loc":"https://saket-choudhary.me/a-frequent-inequality.html"},{"title":"Functions of independent random variables are independent","text":"A textbook problem Given \\(X\\) , \\(Y\\) are two independent random variables, show that functions \\(g(X)\\) and \\(h(Y)\\) are independent too Short Proof Never took a course on measure theory, so avoiding that route: Let. \\(R = g(X)\\) \\(S = h(Y)\\) Also define, \\(A_r = \\{x: g(X)\\leq r\\}\\) and \\(B_s = \\{y:h(Y) \\leq s\\}\\) $$ F_{RS}(r,s) = P(R\\leq r, S \\leq s) = P(X \\in A_r, Y \\in B_s) $$ Now, since \\(X\\) , \\(Y\\) are independent: $$ P(X \\in A_r, Y \\in B_s) = P(X \\in A_r)P(Y \\in B_s) = F_R(r)F_S(s) $$ Which implies $$ F_{RS}(r,s) = F_R(r)F_S(s) $$ and hence \\(g(X), h(Y)\\) are independent.","tags":"misc","url":"https://saket-choudhary.me/functions-of-independent-random-variables-are-independent.html","loc":"https://saket-choudhary.me/functions-of-independent-random-variables-are-independent.html"},{"title":"Bernoulli random varlable and UMVUE","text":"Problem Let \\(X_1,...,X_n\\) random sample \\(X\\) ~ \\(Bernoulli(p)\\) . For \\(n\\geq 4\\) show that the product \\(X_1X_2X_3X_4\\) is a unbiased estimator for \\(p&#94;4\\) , and use this fact for find the best unbiased estimator of \\(p&#94;4\\) Solution As posted on stackexchange [ToDo: Add variance, prove \\(E[\\phi(T)]=0\\) ] T is a complete sufficient statistic for \\(p\\) . Now, consider indicator \\(I_{X_1=1,X_2=1,X_3=1,X_4=1}\\) which is an unbiased estimator of \\(p&#94;4\\) Rao-Blackwellising: \\( \\begin{align}\\phi(T) &= E[I_{X_1=1,X_2=1,X_3=1,X_4=1}|T] \\\\&=P(X_1=1,X_2=1,X_3=1,X_4=1|T=t) \\\\&=\\frac{P(X_1=1,X_2=1,X_3=1,X_4=1,X_1+X_2+X_3+X_4+\\dots X_n=t)}{P(X_1+X_2+\\dots +X_n=t)} \\\\&= \\frac{P(X_1=1,X_2=1,X_3=1,X_4=1)\\times P(X_5+X_6+\\dots X_n=t-4)}{P(X_1+X_2+\\dots +X_n=t)} \\\\&= \\frac{p&#94;4\\times \\binom{n-4}{t-4}p&#94;{t-4}(1-p)&#94;{n-t}}{\\binom{n}{t}p&#94;{t}(1-p)&#94;{n-t}} \\\\&= \\frac{\\binom{n-4}{t-4}}{\\binom{n}{t}} \\\\&=\\frac{t(t-1)(t-2)(t-3)}{n(n-1)(n-2)(n-3)} \\end{align} \\) Check: $E[\\phi(T)] = 0 $ using moments from here","tags":"misc","url":"https://saket-choudhary.me/bernoulli-random-varlable-and-umvue.html","loc":"https://saket-choudhary.me/bernoulli-random-varlable-and-umvue.html"},{"title":"Distribution of one random variable less than other","text":"Problem \\(P(Y<X)\\) for any two independent random variables \\(X,Y\\) Solution We just follow the definition: \\(P(Y<X) = \\int_{-\\infty}&#94;{\\infty}f_X(x)dx \\int_{-\\infty}&#94;{x}f_Y(y)dy\\) \\(P(Y <X) = \\int F_X(y)f_X(x)dx\\)","tags":"misc","url":"https://saket-choudhary.me/distribution-of-one-random-variable-less-than-other.html","loc":"https://saket-choudhary.me/distribution-of-one-random-variable-less-than-other.html"},{"title":"[Proof]Weighted AM-GM","text":"We make use of Jensen's Inequality and the fact that \\(log(x)\\) is a concave function: For concave f: \\(f(\\frac{\\sum a_ix_i}{\\sum a_i}) \\geq \\frac{\\sum a_i f(x_i)}{\\sum a_i}\\) \\(f=log(X)\\) \\(log(\\frac{\\sum a_ix_i}{\\sum a_i}) \\geq \\frac{\\sum a_i log(x_i)}{\\sum a_i}\\) \\(\\implies\\) \\(log(\\frac{\\sum a_ix_i}{\\sum a_i}) \\geq log(\\prod (x_i)&#94;{\\frac{1}{a_i}})\\) Thus, \\(\\frac{\\sum_i a_i x_i}{\\sum_i a_i} \\geq (\\Pi x_i&#94;{a_i})&#94;{\\frac{1}{\\sum_i a_i}}\\)","tags":"misc","url":"https://saket-choudhary.me/proofweighted-am-gm.html","loc":"https://saket-choudhary.me/proofweighted-am-gm.html"},{"title":"Expected number of pairings | Jones and Smith Family","text":"Problem 4 members of Smith family and 4 of Jones family are pooled to form all possible pairs, all equally likely. Find \\(N\\) , the number of smiths which have a smith partner. Solution \\(X= \\sum_{i=1}&#94;4 I_i\\) Where \\(I_i=1\\) iff \\(i&#94;{th}\\) meber of smith family has some smith member as it's partner Number of ways \\(i&#94;{th}\\) smith member can have a smith partner (Choose 1 from remaining 3 Smiths, form group of the remaining 6 ) $ = \\binom{3}{1} \\times \\frac{6!}{(2!)&#94;3} $ Hence, \\(P(I_i=1)= \\frac{\\binom{3}{1}\\frac{6!}{(2!)&#94;3}}{\\frac{8!}{(2!)&#94;4}} = \\frac{3}{14}\\) The trickery happens because at the next step I am going to over count, since the wording in the question is a bit ambiguous and we are concerned with the number of smiths have a partner. I should have multiplied \\(P(I_i=1)\\) by \\(4\\) and then divided by 2, but I am going to overcount with ambiguity. \\(N = 4 \\times \\frac{3}{14} = \\frac{6}[7]\\) which ofcourse is not an integer, but did we expect it to?","tags":"misc","url":"https://saket-choudhary.me/expected-number-of-pairings-jones-and-smith-family.html","loc":"https://saket-choudhary.me/expected-number-of-pairings-jones-and-smith-family.html"},{"title":"Poisson Demysitified","text":"Learnt it the hard way. This one is more of a scratch pad, inspired from the previously discussed problme on estimating the size of restriction fragments. Notations: \\(N_t\\) = Number of events that have happened in the \\(t&#94;{th}\\) time interval \\(T\\) = Time taken till the first arrival occurs(or the next arrival occurs since waiting time is memory less i.e$)i Now consider the event, \\(\\{T > x \\}\\) so you had to wait \\(x\\) seconds before your first bus came and the buses wre coming at a rate of \\(\\lambda\\) per second. \\(\\{T > x\\} \\sim N_1 = 0\\)","tags":"misc","url":"https://saket-choudhary.me/poisson-demysitified.html","loc":"https://saket-choudhary.me/poisson-demysitified.html"},{"title":"Prove: No UMVUE exists for $\\frac{1}{\\theta}$ for Poisson Distribution.","text":"Problem Proof: No UMVUE exists for \\(\\frac{1}{\\theta}\\) for Poisson Distribution. Solution http://stats.stackexchange.com/a/152660/11668 Observation 1: \\(\\sum X_i\\) is complete and sufficient statistic for \\(\\theta\\) Observation 2: \\(\\sum X_i \\sim Poisson(n\\theta)\\) We need to look for an unbiased estimator of \\(\\frac{1}{\\theta}\\) in order to utilise Rao-Blackwell theorem Let \\(\\delta(x)\\) be such an estimator: \\(E[\\delta(x)] = \\frac{1}{\\theta}\\) Then, \\(\\sum_{k=0}&#94;{\\infty}\\frac{\\delta(k)e&#94;{-\\theta}\\theta&#94;k}{k!} = \\frac{1}{\\theta}\\) \\(\\implies\\) \\(\\sum_{k=0}&#94;{\\infty}\\frac{\\delta(k)\\theta&#94;k}{k!} = \\frac{e&#94;\\theta}{\\theta}\\) By Taylor expansion: \\(e&#94;\\theta = \\sum_{k=0}&#94;{\\infty}\\frac{\\theta&#94;k}{k!}\\) \\(\\implies\\) \\(\\frac{e&#94;\\theta}{\\theta} = \\sum_{k=0}&#94;{\\infty}\\frac{\\theta&#94;{k-1}}{k!}\\) Thus, \\(\\sum_{k=0}&#94;{\\infty}\\frac{\\delta(k)\\theta&#94;k}{k!} =\\sum_{k=0}&#94;{\\infty}\\frac{\\theta&#94;{k-1}}{k!} \\implies \\delta(k) = \\frac{1}{\\theta}\\) which is a useless estimator Since, there is no unbiased estimator, there exists no UMVUE as well.","tags":"misc","url":"https://saket-choudhary.me/prove-no-umvue-exists-for-frac1theta-for-poisson-distribution.html","loc":"https://saket-choudhary.me/prove-no-umvue-exists-for-frac1theta-for-poisson-distribution.html"},{"title":"Restriction Fragments, MLE,  and Mixed Random Variables","text":"Let \\(X_1, X_2, \\dots , X_n\\) be the lengths of \\(n\\) restriction fragments. Suppose that a biotechnique can measure fragment lengths accurately up to a given length c. That is, if \\(X_i < c\\) , then the technique gives correct value \\(X_i\\) Show that MLE of \\(\\lambda = \\frac{n-T_n}{S_n+cT_n}\\) where \\(S_n= \\sum_{j=1}&#94;n X_jI(X_j < c)\\) and \\(T_n = \\sum_{j=1}&#94;n I(X_j \\geq c)\\) Solution This is another of my favorite problems that requires clever use of exponential variables. I also answered a similar question on Mathematics stackexchange Few observations: \\(P(X_i < x) = 1 - \\lambda e&#94;{-x}\\) if \\( 0 \\leq x < c\\) \\(P(X_i = c) = P(X_i \\geq c) = e&#94;{-\\lambda c}\\) (This one looks counter intuitive at first look, but that is what the \\(X_i=c\\) if \\(X_i \\geq c\\) returns) Now, \\( \\begin{align}L( \\lambda| x_i) &= \\prod_{i=1}&#94;{n} (\\lambda e&#94;{-\\lambda x_i}) I(0 \\leq x_i < c) \\times (e&#94;{-\\lambda c})I( x_i \\geq c) \\\\&= \\lambda&#94;{\\sum_{i=1}&#94;n I(0 \\leq x_i < c)}e&#94;{-\\lambda(\\sum_{i=1}&#94;n x_iI(0 \\leq x_i < c)} \\times e&#94;{-\\lambda c \\sum_{i=1}&#94;n I(x_i \\geq c)} \\\\&= \\lambda&#94;{n-T_n}e&#94;{-\\lambda S_n} \\times e&#94;{-\\lambda c T_n}\\end{align} \\) \\(\\log L= (n-T_n) \\log \\lambda -\\lambda(S_n+cT_n)\\) \\(\\frac{\\partial \\log L}{\\partial \\lambda} = \\frac{n-T_n}{\\lambda}-(S_n+cT_n)\\) which gives \\(\\hat \\lambda = \\frac{n-T_n}{S_n+cT_n}\\)","tags":"misc","url":"https://saket-choudhary.me/restriction-fragments-mle-and-mixed-random-variables.html","loc":"https://saket-choudhary.me/restriction-fragments-mle-and-mixed-random-variables.html"},{"title":"Restriction Fragments Size Distribution","text":"Given The number of restriction sites in a fragment of length \\(t\\) are distributed as: $$ P(N_t=k)=e&#94;{-\\lambda t}\\frac{(\\lambda t)&#94;k}{k!} $$ Find the distribution of length of restriction fragments Solution Intuition: If I have a restriction site at some location \\(z\\) and want it to be atleast \\(x\\) base pairs long, i.e. I have a fragment starting at \\(z\\) and going atleast till \\(z+x\\) , I need to ensure I do not encounter any other restriction site in between, which is also equivalent to \\(P(N_x=0) = e&#94;{-\\lambda x}\\) Thus, the probability that the restriction site is atleast \\(x\\) bp is given by: $$ P(X > x) = e&#94;{-\\lambda x} $$ and hence, $$ P(X < x) = 1-e&#94;{-\\lambda x} \\implies P(X=x) = \\lambda e&#94;{-\\lambda x} $$ Thus, the size of restriction fragments follows an exponential distribution.","tags":"misc","url":"https://saket-choudhary.me/restriction-fragments-size-distribution.html","loc":"https://saket-choudhary.me/restriction-fragments-size-distribution.html"},{"title":"Convolution Demysitifed","text":"Problem Given \\(f(x) = \\frac{x}{2}\\) for \\(0 \\leq x \\leq 2\\) . Find the pdf of \\(x_1+x_2\\) for \\(x_1,x_2\\) which are i.i.d. Wrong solution $$ f*g(t) = \\int_{-\\infty}&#94;{\\infty} f(w)g(t-w)dw $$ Thus, blindly, \\(f_S(s) = \\int_{0}&#94;s \\frac{x(s-x)}{4}dx = \\frac{s&#94;3}{24}\\) and \\(0 \\leq s \\leq 4\\) But is \\(f_S(s)\\) a PDF? No. \\(\\int_0&#94;4 f_S(s)ds = \\frac{4&#94;4}{96} \\neq 1\\) Respecting the bounds. \\(f_S(s) = \\int_{0}&#94;s \\frac{x}{2} \\frac{s-x}{2}dx\\) for : $$ 0 \\leq x \\leq 2 $$ and $$ 0 \\leq s-x \\leq 2 $$ Which gives us, these bounds: \\(s-2 \\leq x \\leq s\\) and \\(0 \\leq x \\leq 2\\) In order to respect the above bounds, \\(f*g(s) = \\int_{-\\infty}&#94;{\\infty}f(x)f(s-x)dx\\) But, \\(f(x) \\neq 0\\) for \\(0 \\leq x \\leq 2\\) and \\(f(s-x) \\neq 0\\) for \\(s-2 \\leq x \\leq s\\) thus, \\(f(x)f(s-x) \\neq 0\\) for \\(max(0,s-2) \\leq x \\leq min(s,2)\\) Range of s: \\([0,4]\\) \\(f*g(s) = \\int_{max(0,s-2)}&#94;{min(s,2)}f(x)f(s-x)dx\\) For \\(0 \\leq s \\leq 2\\) : \\(min(s,2)=s\\) and \\(max(0,s-2)=0\\) For \\(2 \\leq s \\leq 4\\) : \\(min(s,2)=2\\) and \\(max(0,s-2)=s-2\\) Now, $$ f_S(s) = \\begin{cases} \\int_{0}&#94;{s}\\frac{x}{2}\\frac{s-x}{2}dx = \\frac{s&#94;3}{24} & 0 \\leq s \\leq 2\\\\ \\int_{s-2}&#94;{2}\\frac{x}{2}\\frac{s-x}{2}dx = \\frac{1}{4}(\\frac{s(2&#94;2-(s-2)&#94;2)}{2}-\\frac{2&#94;3-(s-2)&#94;3}{3}) & 2 \\leq s \\leq 4\\\\ \\end{cases} $$","tags":"misc","url":"https://saket-choudhary.me/convolution-demysitifed.html","loc":"https://saket-choudhary.me/convolution-demysitifed.html"},{"title":"MATH 501 Project","text":"Introduction Model of a two state reparable system: $$ \\begin{equation} \\frac{dp_0}{dt} = -\\lambda_0 p_0(t) + \\int_0&#94;1 \\mu_1(x)p_1(x,t)dx + \\int_0&#94;1u&#94;{*}(x,t)dx \\end{equation} $$ $$ \\begin{equation} \\frac{\\partial p_1(x,t)}{\\partial t} + \\frac{\\partial p_1(x,t)}{\\partial x} = -\\mu_1(x)p_1(x,t) - u&#94;{*}(x,t) \\end{equation} $$ where: \\(p_0(t)\\) : Probability that the device is in good mode 0 at time \\(t\\) . \\(p_1(x,t)\\) : Probability density (with respect to repair time \\(x\\) ) that the failed device is in failure mode 1 at time \\(t\\) and has an elapsed repair time of \\(x\\) \\(\\mu_1(x)\\) : Time-dependent nonnegative repair rate when the device is in failure state and has an elapsed repair time of \\(x\\) . Carefully provide a mathematical description of the problem discussed in this report. $$ \\begin{equation} \\frac{dp_0}{dt} = -\\lambda_0 p_0(t) + \\int_0&#94;1 \\mu_1(x)p_1(x,t)dx + \\int_0&#94;1u&#94;{*}(x,t)dx \\end{equation} $$ $$ \\begin{equation} \\frac{\\partial p_1(x,t)}{\\partial t} + \\frac{\\partial p_1(x,t)}{\\partial x} = -\\mu_1(x)p_1(x,t) - u&#94;{*}(x,t) \\end{equation} $$ Given Initial Conditions $$ \\begin{eqnarray} p_1(x,0) &=& 0\\\\ p_0(0) &=& 1 \\end{eqnarray} $$ Given Boundary Conditions: $$ \\begin{eqnarray} p_1(0,t) &=& \\lambda_0 p_0(t)\\\\ p_1(1,t) &=& 0 \\end{eqnarray} $$ The function \\(u&#94;*\\) is given by $$ \\begin{eqnarray} u&#94;{*}(x,t) &=& (0.3+0.1 sin(x))b(t) \\end{eqnarray} $$ The function \\(b(t)\\) represents the input. It is related to the cost constraint function \\(c(t)\\) as given below. $$ \\begin{eqnarray} b(t)+\\int_0&#94;1\\mu_1(x)f(x,t)dx -0.3p_0&#94;{*}(t) = c(t)\\\\ f(x,t) = 0.1\\cos(\\pi t)\\sin&#94;2(1-x) \\end{eqnarray} $$ Our objective is to find the input \\(b(t)\\) such that the resulting distribution \\(p_0(t)\\) is closest (as measured by the 2-norm) to the optimal distribution \\(p_0&#94;*(t)\\) given below. $$ \\begin{equation} p_0&#94;{*}(t) = 0.85+0.05\\cos(2\\pi t) \\end{equation} $$ Methodology We make couple of substitutions, following the notation that \\(z_i&#94;j\\) refers to the value of \\(z\\) evaluated at time point \\(i\\) and at position \\(j\\) . The repair time is divided into \\(m\\) subintervals, while the system running time is divided into \\(n\\) subintervals. For the purposes of numerical implementation, we chose \\(m\\) and \\(n\\) to be \\(20\\) and \\(400\\) respectively. $$ \\begin{align}p_0(t_j) &= v_j \\quad 0 \\le j \\le n\\\\ p_1(x_i,t_j) &= w_j&#94;i \\quad 0 \\le i \\le m, \\, 0\\le j \\le n\\\\ \\mu_1(x_i) &= \\mu&#94;i \\quad 0 \\le i \\le m\\\\ \\lambda &= \\lambda_0 \\\\ \\end{align}$$ Using the new notation, the boundary conditions and initial conditions may be written as follows. Initial conditions: $$\\begin{eqnarray} w_0&#94;{i} &=& 0 \\quad \\forall 0\\le i \\le m\\\\ v_0 &=& 1 \\end{eqnarray}$$ Boundary Conditions: $$\\begin{eqnarray} w_j&#94;{20} &=& 0 \\quad \\forall 0\\le j\\le n\\\\ w_j&#94;0 &=& \\lambda v_j \\end{eqnarray}$$ Also condensing, $$\\begin{eqnarray} I_j&#94;{*}=u&#94;{*}(x_i,t_j) &=& g&#94;ib_j \\\\ \\int_0&#94;1 u&#94;{*}(x,t_j)dx = \\alpha b_j\\\\ where\\ g&#94;i = (0.3+0.1sin(x&#94;i)) \\nonumber \\end{eqnarray}$$ And, $$\\begin{eqnarray} b(t)+\\int_0&#94;1\\mu_1(x)f(x,t)dx -0.3p_0&#94;{*}(t) = c(t) \\nonumber \\\\ b_j = c_j - f_j \\\\ where\\ f_j = \\int_0&#94;1\\mu_1(x)f(x,t_j)dx -0.3p_0&#94;{*}(t_j) \\end{eqnarray}$$ Discretizing $$\\begin{align} \\frac{v_{j+1}-v_j}{\\tau} &= -\\lambda v_j + I_j + I_j&#94;{*} \\\\ I_j &= h[\\frac{\\mu&#94;0 w_j&#94;0}{2} + \\sum_{k=1}&#94;{19} \\mu&#94;k w_j&#94;k + \\frac{\\mu&#94;{20} w_j&#94;{20}}{2} ] \\\\ &= h[\\frac{\\mu&#94;0 w_j&#94;0}{2} + \\sum_{k=1}&#94;{19} \\mu&#94;k w_j&#94;k ] \\nonumber \\\\ I_j&#94;{*} &= \\alpha b_j \\end{align}$$ Discretizing $$ \\begin{align} \\frac{w_{j+1}&#94;i-w_{j}&#94;i}{\\tau} + \\frac{w_j&#94;{i+1}-w_j&#94;{i-1}}{2h} = -\\mu&#94;iw_j&#94;i - g&#94;ib_j \\nonumber\\\\ w_{j+1}&#94;i=w_{j}&#94;i-\\frac{\\tau}{2h}(w_j&#94;{i+1}-w_j&#94;{i-1})-\\tau\\mu&#94;i w_j&#94;i - \\tau g&#94;i b_j \\end{align} $$ Applying LAX scheme $$w_j&#94;i = \\frac{w_j&#94;{i-1} + w_j&#94;{i+1}}{2}$$ we get, $$ \\begin{align*} w&#94;i_{j+1} =& \\left( \\frac{w_j&#94;{i+1} + w_j&#94;{i-1}}{2} \\right) -\\frac{\\tau}{2h}(w_j&#94;{i+1}-w_j&#94;{i-1}) \\\\ &-\\tau\\mu&#94;i\\left( \\frac{w_j&#94;{i+1} + w_j&#94;{i-1}}{2} \\right) \\\\ &- \\tau g&#94;i b_j\\\\ w&#94;i_{j+1} =& \\frac{1}{2}\\left( 1-\\mu&#94;i\\tau + \\frac{\\tau}{h} \\right) w&#94;{i-1}_j + \\\\ & \\frac{1}{2}\\left( 1-\\mu&#94;i\\tau - \\frac{\\tau}{h} \\right) w&#94;{i+1}_j - \\\\ & \\tau g&#94;i b_j \\end{align*} $$ Under an appropriately defined matrix \\(A\\) , we can re-write the above equation to read $$ \\begin{align} \\vec{w}_{j+1} =& A\\vec{w}_j - b_j\\tau\\vec{g} + \\vec{e_1}v_{j+1} \\\\ =& (A)&#94;{j+1} \\vec{w}_0 - \\left[ \\sum_{k=0}&#94;j b_k (A)&#94;{j-k} \\right]\\vec{g}\\tau \\\\ &+ \\left[ \\sum_{k=0}&#94;j v_{k+1}(A)&#94;{j-k} \\right]\\vec{e_1} \\nonumber \\end{align} $$ where \\(\\vec{e_1}\\) is an \\(m\\times1\\) matrix given by $$ \\begin{align} \\vec{e_1} = \\left[ \\lambda,0,\\dots,0 \\right]&#94;T \\end{align} $$ Matrix A has the form: $$ \\begin{align} %\\[ \\begin{pmatrix} w&#94;0_{j+1} \\\\ w&#94;1_{j+1} \\\\ \\cdots \\\\ w&#94;{n-2}_{j+1} \\\\ w&#94;{n-1}_{j+1} \\\\ w&#94;{n}_{j+1} \\end{pmatrix} &= \\left( \\begin{matrix} 0 & 0 & 0 & \\cdots & 0 & 0 \\\\ %1 & a_{1} & 0 & a_{3} & \\cdots & 0 & 0\\\\ 0 & a_{2} & 0 & a_{4} & \\cdots & 0\\\\ 0 & 0 & a_{3} & 0 & a_5 & 0\\\\ \\vdots \\\\ 0 & 0 & \\cdots & 0 & 0 & 0 \\end{matrix} \\right) \\begin{pmatrix} w&#94;0_j \\\\ w&#94;1_j \\\\ \\cdots \\\\ w&#94;{n-2}_j \\\\ w&#94;{n-1}_j \\\\ w&#94;{n}_j\\end{pmatrix} \\\\ &+ b\\tau \\begin{pmatrix} g&#94;0 \\\\ g&#94;1 \\\\ \\vdots \\\\ g&#94;{n-2} \\\\ g&#94;{n-1} \\\\ g&#94;{n} \\end{pmatrix} + v_{j+1} \\begin{pmatrix} \\lambda \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\end{align} $$ From $$ \\begin{align} v_{j+1} &= (1-\\lambda \\tau)v_j + \\tau I_j + \\tau I_j&#94;{*} \\\\ &= (1-\\lambda \\tau + \\frac{h\\tau}{2})v_j + h\\tau\\vec{\\mu}&#94;T\\vec{w}_j + \\alpha b_j\\tau %v_{j+1} = av_j + \\tau I_j + \\tau I_j&#94;{*}\\\\ %v_{j} = av_{j-1} + \\tau I_{j-1} + \\tau I_{j-1}&#94;{*}\\\\ %v_{j+1} = a&#94;2v_{j-1} + \\tau I_j + a\\tau I_{j-1} + \\tau I_{j}&#94;{*} + a\\tau I_{j-1}&#94;{*} \\end{align}$$ Substitute the expression for the time evolution for \\(\\vec{w}\\) in the above to obtain, $$\\begin{align*} v_{j+1} =& (1-\\lambda \\tau+ \\frac{h\\tau}{2})v_j \\\\ & + h\\tau\\vec{\\mu}&#94;T(A)&#94;{j}\\vec{w}_0 \\\\ & - \\vec{\\mu}&#94;T\\left[ \\sum_{k=0}&#94;{j-1} b_{k}(A)&#94;{j-1-k} \\right]\\vec{g}\\tau\\\\ & + \\alpha b_j\\tau \\end{align*}$$ Let's define $$\\begin{align} \\beta_{j,k} &= \\vec{\\mu}&#94;T (A)&#94;{j-1-k}\\vec{g} \\\\ \\omega_j &=\\vec{\\mu}&#94;T(A)&#94;{j}\\vec{w}_0 \\\\ \\gamma &=(1-\\lambda \\tau+ \\frac{h\\tau}{2}) \\end{align} $$ $$ \\begin{align} \\vec{\\beta_0} = \\begin{pmatrix} \\alpha \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\\\ \\end{align} $$ $$ \\begin{align} v_{j+1} &= \\gamma v_j + h\\tau\\omega_j - \\tau\\sum_{k=0}&#94;{j-1}\\beta_{j,k}b_k + \\alpha b_j\\tau \\\\ &= \\gamma v_j + h\\tau\\omega_j - \\tau \\vec{\\beta}_j&#94;T \\vec{b}\\\\ &= \\gamma&#94;{j+1}v_0 + h\\tau\\sum_{k=0}&#94;{j} \\gamma&#94;{j-k}\\omega_k - \\left(\\sum_{k=0}&#94;{j} \\gamma&#94;{j-k} \\vec{\\beta}_k\\right)&#94;T \\vec{b} \\end{align} $$ Under appropriately defined strictly lower triangular matrices \\(G\\) and \\(B\\) , $$ \\begin{align} \\vec{v} = - B\\vec{b}+\\vec{d} + h\\tau G\\vec{\\omega} \\\\ \\vec{d}=\\begin{pmatrix} 1 \\\\ \\gamma \\\\ \\gamma&#94;2 \\\\ \\gamma&#94;3 \\\\ \\vdots \\\\ \\gamma&#94;{n} \\end{pmatrix}v_0 \\end{align} $$ where row \\(j+1\\) of matrix \\(B\\) is given by $$\\begin{equation} \\left(\\sum_{k=0}&#94;{j} \\gamma&#94;{j-k} \\vec{\\beta}_k\\right)&#94;T \\end{equation}$$ From the initial conditions, we have \\(\\vec{w_0} = \\vec{0}\\) Thus \\(\\vec{\\omega} = \\vec{0}\\) . Consequently, we have, $$ \\begin{align} \\vec{v} &= -B\\vec{b} + \\vec{d} \\end{align} $$ Note that \\(\\vec{d}\\) is a known quantity. Hence our optimization problem reduces to finding \\(\\vec{b}\\) that best fits the equation $$ \\begin{align} B\\vec{b} = \\vec{d}-\\vec{v}&#94;* \\end{align} $$ The matrix \\(B\\) is invertible in this case and hence our estimate of \\(\\vec{v}\\) is given by $$ \\begin{align} \\hat{\\vec{b}} &= B&#94;{-1} \\left( \\vec{d} - \\vec{v}&#94;* \\right)\\\\ \\vec{v} &= -B\\hat{\\vec{b}} + \\vec{d} \\\\ &= \\vec{v}&#94;* \\end{align} $$ We approximate the integral using: $$ \\begin{equation} \\int_{-1}&#94;{1}f(x,t)dx = \\sum_{i=1}&#94;{n} \\rho_if(x_i,t) \\end{equation} $$ We used Gauss-Legendre quadrature with \\(n=4\\) . Thus, $$ \\begin{align} \\int_{0}&#94;{1}f(x,t)dx &= \\frac{1}{2}\\sum_{i=1}&#94;{n} \\rho_if\\left(\\frac{x_i}{2} + \\frac{1}{2},t\\right)\\\\ \\therefore c_j &= b_j + f_j \\end{align} $$ Observation and Conclusions The inversion of matrix \\(B\\) does present some difficulties. It is easily seen from the definition of \\(B\\) that it is lower triangular. In addition, $$ \\begin{equation} (B)_{ii} = \\alpha\\tau \\quad \\forall 0\\le i \\le n \\end{equation} $$ As \\(B\\) is lower triangular, its determinant is the product of the main diagonal terms. $$ \\begin{equation} |B| = (\\alpha\\tau)&#94;{n+1} \\end{equation} $$ We have \\(\\alpha \\approx 0.354\\) , \\(\\tau = 0.025\\) and \\(n=401\\) . Thus, $$ \\begin{equation} |B| \\approx 10&#94;{-802} \\end{equation} $$ which makes \\(B\\) dangerously close to being singular. In fact, taking a direct inverse in Julia results in multiple entries going to $$\\texttt{NaN}$$ . We resolve this issue by computing the Moore-Penrose pseudoinverse \\(B&#94;+\\) of \\(B\\) . For an invertible matrix \\(B\\) , \\(B&#94;- = B&#94;+\\) m = 21 # The number of 'space' nodes n = 401 # The number of 'time' nodes τ = 10 / ( n - 1 ) # Time step size h = 1 / ( m - 1 ) # Space step size λ = 0.3 α = 0.3 + 0.1 * ( 1 - cos ( 1 )) # Integral of g ( x ) = 0.3 + 0.1 sin ( x ) = 0.3 + 0.1 ( 1 - cos1 ) x = vec ( linspace ( 0 , 1 , m )) t = vec ( linspace ( 0 , 10 , n )) α = 0.3 + 0.1 * ( 1 - cos ( 1 )) # Integral of g ( x ) = 0.3 + 0.1 sin ( x ) = 0.3 + 0.1 ( 1 - cos1 ) μ = 1. / ( 1 - x ). &#94; 2 ; μ [ 1 ] = 0 ; μ [ m ] = 0 ; β = zeros ( m , m ) γ = ( 1 - λ * τ + h * τ / 2 ) optv = 0.85 + 0.05 cos ( 2 π * t ) # The optimal distribution we are aiming at ## Calcuate A A = zeros ( m , m ) for i = 2 : m - 1 A [ i,i-1 ] = 1 - τ * μ [ i ] + τ / h A [ i,i+1 ] = 1 - τ * μ [ i ] - τ / h end ## Calculate g g = ( 0.3 + 0.1 * sin ( x )) ## Quadrature function mu ( x ) return 1 / ( 1 - x ) &#94; 2 end function f ( x ) ## Use just the x part ## Remaining cos ( πt ) should be multiplied later return 0.1 * sin ( 1 - x ) * sin ( 1 - x ) end function fmu ( x ) return f ( x ) mu ( x ) end weights = [ (18+√30)/36,(18-√30)/36 ] point1 = √ ( 3 - 2 ( √ 6 ) / ( √ 5 )) / √ 7 point2 = √ ( 3 + 2 ( √ 6 ) / ( √ 5 )) / √ 7 ## Integrate wrt x fx = weights [ 1 ]* fmu (( point1 + 1 ) / 2 ) + weights [ 1 ]* fmu (( - point1 + 1 ) / 2 ) + weights [ 2 ]* fmu (( point2 + 1 ) / 2 ) + weights [ 2 ]* fmu (( - point2 + 1 ) / 2 ) fx = fx / 2 ## Multiply by time vector fxt = fx * cos ( π * t ) ## Calculate β β = zeros ( n , n ) β [ 1,1 ] = α for j = 2 : n for k = 1 : j - 1 a = ( A &#94; ( j - 1 - k )) '*μ β[j,k] = dot(μ,g)[1] end β[j,j] = α end B = zeros(n,n) B[1,1] = α for j=2:n for k=1:j B[j,:] += γ&#94;(j-k)*β[k,:] end end B = τ*B d = zeros(n,1) for i=1:n d[i] = γ&#94;(i-1)*0.9 end estb = pinv(B)*vec(d-optv) estv = -B*estb + d errv = norm(estv-optv,2); c = estb + fxt - 0.3optv w = zeros(m,n) w[1,:] = λ*estv w[1,1] = 0 for i=2:n w[:,i] = A*w[:,i]-τ*estb[i]*g end M = ones(m) p1hat = M' * w * h using Gadfly plot ( x = t , y = estv , Guide . XLabel ( \"Time\" ), Guide . YLabel ( \"Estimated p0\" ), Guide . XTicks ( ticks =[ 0:1:10 ] )) id=\"fig-c22dfd48538b4aa0ba3f8e349406edc7\"> Time 0 1 2 3 4 5 6 7 8 9 10 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 0.54 0.55 0.56 0.57 0.58 0.59 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 1.05 1.06 1.07 1.08 1.09 1.10 1.11 1.12 1.13 1.14 1.15 1.16 0.4 0.6 0.8 1.0 1.2 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.08 1.10 1.12 1.14 1.16 Estimated p0 </clipPath plot(x=t, y=abs(estv-optv), Guide.XLabel(\"Time\"), Guide.YLabel(\"Absolute Error\"),Guide.XTicks(ticks=[0:1:10])) id=\"fig-a9ec3c81e95b4f6b839742ba6960be33\"> Time 0 1 2 3 4 5 6 7 8 9 10 -1.2×10⁻⁵ -1.0×10⁻⁵ -8.0×10⁻⁶ -6.0×10⁻⁶ -4.0×10⁻⁶ -2.0×10⁻⁶ 0 2.0×10⁻⁶ 4.0×10⁻⁶ 6.0×10⁻⁶ 8.0×10⁻⁶ 1.0×10⁻⁵ 1.2×10⁻⁵ 1.4×10⁻⁵ 1.6×10⁻⁵ 1.8×10⁻⁵ 2.0×10⁻⁵ 2.2×10⁻⁵ -1.00×10⁻⁵ -9.50×10⁻⁶ -9.00×10⁻⁶ -8.50×10⁻⁶ -8.00×10⁻⁶ -7.50×10⁻⁶ -7.00×10⁻⁶ -6.50×10⁻⁶ -6.00×10⁻⁶ -5.50×10⁻⁶ -5.00×10⁻⁶ -4.50×10⁻⁶ -4.00×10⁻⁶ -3.50×10⁻⁶ -3.00×10⁻⁶ -2.50×10⁻⁶ -2.00×10⁻⁶ -1.50×10⁻⁶ -1.00×10⁻⁶ -5.00×10⁻⁷ 0 5.00×10⁻⁷ 1.00×10⁻⁶ 1.50×10⁻⁶ 2.00×10⁻⁶ 2.50×10⁻⁶ 3.00×10⁻⁶ 3.50×10⁻⁶ 4.00×10⁻⁶ 4.50×10⁻⁶ 5.00×10⁻⁶ 5.50×10⁻⁶ 6.00×10⁻⁶ 6.50×10⁻⁶ 7.00×10⁻⁶ 7.50×10⁻⁶ 8.00×10⁻⁶ 8.50×10⁻⁶ 9.00×10⁻⁶ 9.50×10⁻⁶ 1.00×10⁻⁵ 1.05×10⁻⁵ 1.10×10⁻⁵ 1.15×10⁻⁵ 1.20×10⁻⁵ 1.25×10⁻⁵ 1.30×10⁻⁵ 1.35×10⁻⁵ 1.40×10⁻⁵ 1.45×10⁻⁵ 1.50×10⁻⁵ 1.55×10⁻⁵ 1.60×10⁻⁵ 1.65×10⁻⁵ 1.70×10⁻⁵ 1.75×10⁻⁵ 1.80×10⁻⁵ 1.85×10⁻⁵ 1.90×10⁻⁵ 1.95×10⁻⁵ 2.00×10⁻⁵ -1×10⁻⁵ 0 1×10⁻⁵ 2×10⁻⁵ -1.0×10⁻⁵ -9.0×10⁻⁶ -8.0×10⁻⁶ -7.0×10⁻⁶ -6.0×10⁻⁶ -5.0×10⁻⁶ -4.0×10⁻⁶ -3.0×10⁻⁶ -2.0×10⁻⁶ -1.0×10⁻⁶ 0 1.0×10⁻⁶ 2.0×10⁻⁶ 3.0×10⁻⁶ 4.0×10⁻⁶ 5.0×10⁻⁶ 6.0×10⁻⁶ 7.0×10⁻⁶ 8.0×10⁻⁶ 9.0×10⁻⁶ 1.0×10⁻⁵ 1.1×10⁻⁵ 1.2×10⁻⁵ 1.3×10⁻⁵ 1.4×10⁻⁵ 1.5×10⁻⁵ 1.6×10⁻⁵ 1.7×10⁻⁵ 1.8×10⁻⁵ 1.9×10⁻⁵ 2.0×10⁻⁵ Absolute Error </clipPath plot(x=t,y=estb, Guide.XLabel(\"Time\"), Guide.YLabel(\"b(t)\"),Guide.XTicks(ticks=[0:1:10])) id=\"fig-4f8776f47c1745e3b1527a931867061a\"> Time 0 1 2 3 4 5 6 7 8 9 10 -0.0030 -0.0025 -0.0020 -0.0015 -0.0010 -0.0005 0.0000 0.0005 0.0010 0.0015 0.0020 0.0025 -0.00250 -0.00245 -0.00240 -0.00235 -0.00230 -0.00225 -0.00220 -0.00215 -0.00210 -0.00205 -0.00200 -0.00195 -0.00190 -0.00185 -0.00180 -0.00175 -0.00170 -0.00165 -0.00160 -0.00155 -0.00150 -0.00145 -0.00140 -0.00135 -0.00130 -0.00125 -0.00120 -0.00115 -0.00110 -0.00105 -0.00100 -0.00095 -0.00090 -0.00085 -0.00080 -0.00075 -0.00070 -0.00065 -0.00060 -0.00055 -0.00050 -0.00045 -0.00040 -0.00035 -0.00030 -0.00025 -0.00020 -0.00015 -0.00010 -0.00005 0.00000 0.00005 0.00010 0.00015 0.00020 0.00025 0.00030 0.00035 0.00040 0.00045 0.00050 0.00055 0.00060 0.00065 0.00070 0.00075 0.00080 0.00085 0.00090 0.00095 0.00100 0.00105 0.00110 0.00115 0.00120 0.00125 0.00130 0.00135 0.00140 0.00145 0.00150 0.00155 0.00160 0.00165 0.00170 0.00175 0.00180 0.00185 0.00190 0.00195 0.00200 -0.004 -0.002 0.000 0.002 -0.0025 -0.0024 -0.0023 -0.0022 -0.0021 -0.0020 -0.0019 -0.0018 -0.0017 -0.0016 -0.0015 -0.0014 -0.0013 -0.0012 -0.0011 -0.0010 -0.0009 -0.0008 -0.0007 -0.0006 -0.0005 -0.0004 -0.0003 -0.0002 -0.0001 0.0000 0.0001 0.0002 0.0003 0.0004 0.0005 0.0006 0.0007 0.0008 0.0009 0.0010 0.0011 0.0012 0.0013 0.0014 0.0015 0.0016 0.0017 0.0018 0.0019 0.0020 b(t) </clipPath plot(x=t,y=p1hat, Guide.XLabel(\"Time\"), Guide.YLabel(\"p1hat(t)\"), Guide.YTicks(ticks=[0:0.0025:0.0225]), Guide.XTicks(ticks=[0:1:10])) id=\"fig-513a44c54ef244079cf52ea51c3ae086\"> Time 0 1 2 3 4 5 6 7 8 9 10 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 0.0225 p1hat(t) </clipPath","tags":"misc","url":"https://saket-choudhary.me/math-501-project.html","loc":"https://saket-choudhary.me/math-501-project.html"},{"title":"Runs in flips of a coin","text":"This problem happened to be in one of the screening examinations and is my favorite because it demonstrates an application of indicator variables Problem A run is defined as maximal subsequence of consecutive tosses all having the same outcome. So HHHTHHTTH has 5 runs.(HHH,T,HH,TT,H). Let \\(R_n\\) represent the number of runs when a coin is flipped \\(n\\) times. Find: \\(ER_n\\) \\(Var(R_n)\\) Distribution of \\(R_n-1\\) Assume \\(p,q\\) probability of showing H,T respectively. Solution Let \\(I_i\\) be an indicator variable such that \\(I_i=1\\) iff \\(i&#94;{th}\\) and \\((i+1)&#94;{th}\\) tosses are different(One of { \\(HT,TH\\) } ) f runs in \\(n\\) flips is given by: \\(R_n = \\sum_{i=1}&#94;{n-1}I_i\\) Well, not really. The point to realise is that the first toss is always a run! Corrected \\(R_n = \\sum_{i=1}&#94;{n-1}I_i + 1\\) Now \\(E[R_n] = 1+\\sum_{i=1}&#94;{n-1}P[I_i=1]\\) We already know \\(P[I-i=1] = pq + qp\\) (Remeber the two possiblites of HT,TH) and hence 1. \\(ER_n = 1+ (n-1)2pq\\) Check Check! \\(ER_1 = 1\\) (Straightforward correct) \\(ER_2 = 1+2pq\\) \\(P(R_2=2) = pq + qp\\) and \\(P(R_2=1) = p&#94;2 + q&#94;2\\) Thus \\(ER_2 = 2(pq+qp) + 1(p&#94;2+q&#94;2) = 1 +2pq\\) (Using \\(p+q=1\\) ) \\(Var(R_n) = ER_n&#94;2 - (ER_n)&#94;2\\) \\(ER_n&#94;2 = E(1+\\sum I_i)&#94;2 = E\\sum_{i=1}&#94;{n-1}I_i&#94;2 +1+2\\sum_{i=1}&#94;{n-1}I_i = \\sum_{i=1}&#94;{n-1}EI_i&#94;2 + 2\\sum_{i < j} EI_iI_j + 2E\\sum_{i=1}EI_1 = 3\\sum E_i + 2\\sum_{i < j}EI_iI_j +1\\) Now realise that \\(EI_iI_j=P(I_iI_j)\\) is an independent event for \\(|i-j|>1\\) and is equal to \\(P(I_1)&#94;2\\) For \\(|i-j|=1\\) we have \\(P(I_iI_j)=pqp+qpq = p&#94;2q+qp&#94;2\\) and I chose in my summation for \\(j\\) to be less than \\(i\\) so the mod sign is redundant Alright, skipping too many things here. but: \\(Var(R_n) = 4pqn -6pq+2pq(n-2)(n-3) - 4p&#94;2q&#94;2(n-1)&#94;2\\) Check for \\(Var(R_2) = 2pq(1-2pq)\\) and \\(Var(R_1)=0\\) That should be a straightforward \\(R_n-1 \\approx Binomial(n,2pq)\\)","tags":"misc","url":"https://saket-choudhary.me/runs-in-flips-of-a-coin.html","loc":"https://saket-choudhary.me/runs-in-flips-of-a-coin.html"},{"title":"SVD v/s MDS v/s PCA","text":"Principle Component Analysis(PCA) is a relatively more famous than Singular Value Decomposition(SVD) or Multidimensional Scaling(MDS). When I was introduced to the latter two, I was utterly confused trying to figure out what goes in where. SVD Let \\(X_{mxn}\\) data matrix. For an easy to relate example, from bioinformatics, let each row represent a gene, and each column represent single patient. The rows thus give expression profile of gene across patients while the columns represent the expression levels of different genes in each person. Without loss of generality we assume \\(m>n\\) and \\(rank(X)=r <n\\) The singular value decomposition of \\(X_{mxn}\\) is then given by: $$ X_{mxn} = U_{mxr}\\sum_{rxr}V_{nxr}&#94;T $$ \\(U_{nxr}\\) consists of orthornormal eigen vectors of \\(X&#94;TX\\) (nxn) \\(V_{mxr}\\) consists of orthornormal eigen vectors of \\(XX&#94;T\\) (mxm) \\(\\sum_{rxr}\\) denotes a diagonal matrix composed of eigen vectors of \\(X&#94;TX\\) arranged with the largest being located at top left, least at bottom right. \\(\\sum_{ii} = \\lambda_i\\) and \\(\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_r\\) Facts: \\(\\lambda_i \\geq 0\\) because \\(X&#94;TX\\) is a positive definite matrix: \\(yX&#94;TXy = (Xy)(Xy)&#94;T > 0\\) always [See: http://en.wikipedia.org/wiki/Positive-definite_matrix#Characterizations] PCA Let \\(X_c\\) denote the mean centered \\(X\\) , i.e. \\(X_c = I'_{mxm}X_{mxn}\\) where \\(I' = I-\\frac{1}{m}\\) thus from each row we substract the ‘average row' leading to $X_c4 being zer-mean centerered. Def: Gram matrix : \\(X_cX_c&#94;T\\) Def: Covariance Matrix: \\(X_c&#94;TX_c\\) Let's say you are given the original uncente The most ‘common' definition of PCA says : For a given set of \\(n\\) dimensional vectors \\(x_1, x_2, x_3,x_4, \\ldots x_m\\) the \\(p<n\\) principal components are those orthogonal axes onto which the variance retained is maximized and hence Plotting/Running PCA - Calculate \\(X_cX_c&#94;T\\) and calculate \\(U\\) as the eigen vectors of this matrix. Retain the vectors corresponding to top \\(p=2\\) eigenvalues - Calculate project \\(Y=U&#94;TX\\) Given gram matrix of original data \\(K\\) , to obtain gram matrix of mean centered data, cholesky decomposition is not required: \\(K_c = (I-\\frac{1}{n})K(I-\\frac{1}{n}) = K-\\frac{I}{n}K -K\\frac{1}{n} \\frac{I}{n}K\\frac{I}{n}\\) Also realize: \\(X_c = U \\sum V&#94;T\\) and \\(UU&#94;T=I\\) ; \\(VV&#94;T=I\\) Thus, \\(K_c=X_cX_c&#94;T = U\\sum&#94;2U&#94;T\\) PCA is equivalent of doing a SVD: of \\(X_c = U\\sumV&#94;T\\) and then using \\(U\\sum\\) as the principle components MDS Given distance matrix \\(D_{ij}\\) of pairwise distances, what would PCA result into? Assuming the distances were euclidean: $$ D_{ij}&#94;2 = ||x_i-x_j||&#94;2 = ||x_i - \\overline{x} + \\overline{x} -x_j||&#94;2 = ||x_i-\\overline{x}||&#94;2 + ||x_j - \\overline{x}|&#94;2 -2\\langle x_i-\\overline{x}, x_j-\\overline{x}\\rangle = ||x_i-\\overline{x}||&#94;2 + ||x_j - \\overline{x}|&#94;2 + 2[K_c]_{ij} $$ which can be cleverly rewritten as: $$K_c = (I-\\frac{1}{n})\\frac{-D&#94;2}{2}(I-\\frac{1}{n}$$ \\(K_c\\) can now undergo SVD to obtain principal components. which is what happens in MDS too. References [1] http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional/132731#132731 [2] SVD: http://infolab.stanford.edu/~ullman/mmds/ch11.pdf [3] SVD in R: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition [4] Multidimensional Scaling, Patrick J.F. Groenen∗ Michel van de Velden [5] SVD and PCA: http://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca [6] Intuitive Explaination of PCA: http://arxiv.org/pdf/1404.1100.pdf [7] PCA: http://www3.cs.stonybrook.edu/~sael/teaching/cse549/Slides/CSE549_16.pdf [8] PCA: http://www.math.ucsd.edu/~gptesler/283/slides/pca_f13-handout.pdf [9] SVD: http://www.cs.wustl.edu/~zhang/teaching/cs517/Spring12/CourseProjects/SVD.pdf [10] PCA v/s MDS: http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional [11] Gaussian Lernels: https://shapeofdata.wordpress.com/2013/07/23/gaussian-kernels/ [12] Kernel PCA: http://sebastianraschka.com/Articles/2014_kernel_pca.html","tags":"misc","url":"https://saket-choudhary.me/svd-vs-mds-vs-pca.html","loc":"https://saket-choudhary.me/svd-vs-mds-vs-pca.html"},{"title":"Multidimensional Scaling","text":"MDS is a statistical technique to visualize dissimilarity between points. The distances between two pointsin n-dimensions are visualized in 2 dimensions such that it represents the distance in n-dimensions as far as possible. It is important to note that, for MDS, we start of with a ‘distance' matrix and not the coordinate of points. Dissimilarity is ‘similar' to distances in most cases(ignoring the scale). Let \\(\\delta_{i,j}\\) represent the distance between \\(i\\ and\\ j\\) Given \\(n\\) points, the idea is to come up with a set of \\(n * p\\) matrix \\(X\\) such that the distance between two vectors \\(x_i\\) and \\(x_j\\) is given by: $$ d_{i,j}&#94;2(X) = \\sum_{k=1}&#94;{p}(x_{ik}-x_{jk})&#94;2 $$ So if we choose \\(p=1\\) , we wish to visualize everything in single dimension. Let's restrict to the case \\(p=2\\) How do you obtain X? The idea again goes back to the definition of keeping the ‘new' distance \\(d_{i,j}(X)\\) as the original dissimilarity \\(\\delta_{i,j}\\) , this leads to the least square fit kind of regression where the goal is to minimize the ‘residual' error. Note, \\(d_{i,j}\\) is an approximation to \\(\\delta_{i,j}\\) The distance matrix could have come from \\(n\\) points having high-dimensions(say \\(p'>p\\) ), and even though these points cannot be visualized (since we cannot draw \\(p'\\) dimensional map) we draw a \\(p=2\\) dimensional map. The coordinates of original \\(n\\) points in \\(p'\\) need not be known. We can still obtain an ‘equaivalent' set of \\(n\\) points in \\(p=2\\) dimensions such that the \\(2D\\) distance between two points \\(i,j\\) is as close as it can be to the original distance. \\(\\sigma&#94;2 = \\sum_{i=1}&#94;{n}\\sum_{j=1}&#94;{i-1}(\\delta_{ij}-d_{ij}(X))&#94;2\\) This is just one way to define ‘closeness' of \\(\\delta_{ij}\\) with \\(\\d_{ij}\\) . \\(X\\) is obtained by minimizing such functions. Imagine doing ordinary least square fit(multidimensional). Checking if MDS makes sense A quick checkt to see if MDS is good enough is to go back to its definition. The final set of vectors \\(X_{n*p}\\) should be able to communiate the original distance matrix and hence a plot of the \\(\\frac{n{n+1}}{2}\\) points if \\(d_{ij}\\) is plotted against \\(X\\) acis and \\(\\delta_{ij}\\) is plotted along \\(Y\\) axis then you should get a straight line representing \\(Y=X\\) .","tags":"misc","url":"https://saket-choudhary.me/multidimensional-scaling.html","loc":"https://saket-choudhary.me/multidimensional-scaling.html"},{"title":"Gumbel distribution expectation","text":"What If \\(X_1,..,X_n\\) is a random sample with density \\(f(x;\\theta)=e&#94;{-(x-\\theta)}e&#94;{-e&#94;{-(x-\\theta)}}\\) ( \\(x \\in\\mathbb{R}\\) ) and \\(-\\infty<\\theta<\\infty\\) , \\(\\quad\\) i) Find the estimator of \\(\\theta\\) Solution First let's confirm if \\(f(x;\\theta)=e&#94;{-(x-\\theta)}e&#94;{-e&#94;{-(x-\\theta)}}\\) represents a PDF. Transforming \\(t=e&#94;{-({x-\\theta})}\\) : $$\\int_{-\\infty}&#94;{\\infty} e&#94;{-(x-\\theta)}e&#94;{-e&#94;{-(x-\\theta)}}dx = \\int_{0}&#94;{\\infty} e&#94;{-t} dt = 1$$ This is indeed a known distribution Now, using similar transformation( \\(t=e&#94;{-(x-\\theta)}\\) ) for $$E[X]=\\int_{-\\infty}&#94;\\infty xe&#94;{-(x-\\theta)}e&#94;{-e&#94;{-(x-\\theta)}}dx$$ we get $$E[X]=\\int_{0}&#94;\\infty (\\theta-ln (t))e&#94;{-t}dt$$ $$E[X]=\\theta - \\int_{0}&#94;\\infty ln (t)\\ e&#94;{-t}dt$$ Thus, $$ E[X] = \\theta + \\gamma$$ where the second term is Euler Mascheroni constant \\(\\gamma\\)","tags":"misc","url":"https://saket-choudhary.me/gumbel-distribution-expectation.html","loc":"https://saket-choudhary.me/gumbel-distribution-expectation.html"},{"title":"BA = AB = I","text":"To Prove: If $$A_{nxn}$$ and $$B_{nxn}$$ such that AB=I, then BA=I $$AB=I \\implies Rank(A), Rank(B)=n$$ Reason: Rank(AB) $$\\leq$$ min(Rank A, Rank B) so B is a full rank matrix. Now consider B=BI $$\\implies$$ B-B(AB)=0 $$\\implies$$ B-(BA)B=0 $$\\implies$$ (I-BA)B=0 Since B is full rank so I-BA=0. Q.E.D","tags":"misc","url":"https://saket-choudhary.me/ba-ab-i.html","loc":"https://saket-choudhary.me/ba-ab-i.html"},{"title":"L2 norm and spectral radius of symmetric matrix","text":"L2 norm and spectral radius for $$A=A&#94;T$$ L2 inducded norm: NOTE: L2 norm is not the same as Frobenius norm. L2 norm is an \"induced vector\" norm, Frobenius is a \"matrix\" norm. Induced norm: $$$|| . ||$$ defined as ||A|| = $$sup_{||x=1||} ||Ax||$$ Note how ||Ax|| is a vector norm too, while ||A|| is a matrix norm","tags":"misc","url":"https://saket-choudhary.me/l2-norm-and-spectral-radius-of-symmetric-matrix.html","loc":"https://saket-choudhary.me/l2-norm-and-spectral-radius-of-symmetric-matrix.html"},{"title":"Positive definite second derivative","text":"Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be a twice-differentiable function, and let \\(f\\) 's second derivative be continuous. Let \\(f\\) be convex with the following definition of convexity: for any \\(a<b \\in \\mathbb{R}\\) : $$f\\left(\\frac{a+b}{2}\\right) \\leq \\frac{f(a)+f(b)}{2}$$ Prove that \\(f'' \\geq 0\\) everywhere. Solution http://math.stackexchange.com/a/1224986/171836 Given \\(f\\) is a continuous and using the results from this answer , \\(f\\) can be proven to satisfy: \\(f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda)f(x_2)\\ \\forall \\ \\lambda \\in [0,1]\\) Now, by using Taylor's expansion, \\(f''(x)\\) can be written as: $$ f''(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h)+f(x-h)-2f(x)}{h&#94;2} $$ \\(f(\\frac{1}{2}(x+h) + \\frac{1}{2}(x-h)) \\leq \\frac{1}{2}f(x+h) + \\frac{1}{2}f(x-h) \\implies 2f(x) \\leq f(x+h)+f(x-h)\\) or \\(f(x+h)+f(x-h)-2f(x) \\geq 0\\) . Since \\(h&#94;2 \\geq 0\\) and \\(f\\) being twice differentiable, \\(f''(x) \\geq 0\\) follows.","tags":"misc","url":"https://saket-choudhary.me/positive-definite-second-derivative.html","loc":"https://saket-choudhary.me/positive-definite-second-derivative.html"},{"title":"Proof for triangle inequality for case $x+y<0$","text":"\\(-|x| \\leq x \\leq |x|\\) and \\(-|y| \\leq y \\leq |y|\\) \\(\\implies\\) \\(-|x|-|y| \\leq x+y \\leq |x|+|y|\\) \\(\\implies\\) \\(|x+y| \\leq |x| +|y|\\) for any real \\(x,y\\) The last implication comes from the fact: \\(|x| \\leq a \\leftrightarrow -a \\leq x \\leq a\\) for some \\(a \\geq 0\\)","tags":"misc","url":"https://saket-choudhary.me/proof-for-triangle-inequality-for-case-xy0.html","loc":"https://saket-choudhary.me/proof-for-triangle-inequality-for-case-xy0.html"}]};