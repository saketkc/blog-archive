
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />

  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="./theme/pygments/monokai.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">


    <link href="http://saket-choudhary.me/pertinent-blog/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Piddling Pertinent Atom">

    <link href="http://saket-choudhary.me/pertinent-blog/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Piddling Pertinent RSS">

    <link rel="shortcut icon" href="url-to-favicon" type="image/x-icon">
    <link rel="icon" href="url-to-favicon" type="image/x-icon">

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-55540107-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#333333">

<meta name="author" content="Saket Choudhary" />
<meta name="description" content="Principle Component Analysis(PCA) is a relatively more famous than Singular Value Decomposition(SVD) or Multidimensional Scaling(MDS). When I was introduced to the latter two, I was utterly confused trying to figure out what goes in where. SVD Let \(X_{mxn}\) data matrix. For an easy to relate example …if (!document.getElementById(&#39;mathjaxscript_pelican_#%@#$@#&#39;)) { var align = &#34;center&#34;, indent = &#34;0em&#34;, linebreak = &#34;false&#34;; if (false) { align = (screen.width" />
<meta name="keywords" content="math, statistics, ml">

<meta property="og:site_name" content="Piddling Pertinent"/>
<meta property="og:title" content="SVD v/s MDS v/s PCA"/>
<meta property="og:description" content="Principle Component Analysis(PCA) is a relatively more famous than Singular Value Decomposition(SVD) or Multidimensional Scaling(MDS). When I was introduced to the latter two, I was utterly confused trying to figure out what goes in where. SVD Let \(X_{mxn}\) data matrix. For an easy to relate example …if (!document.getElementById(&#39;mathjaxscript_pelican_#%@#$@#&#39;)) { var align = &#34;center&#34;, indent = &#34;0em&#34;, linebreak = &#34;false&#34;; if (false) { align = (screen.width"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./svd-vs-mds-vs-pca.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2015-04-15 00:00:00-07:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/saket-choudhary.html">
<meta property="article:section" content="misc"/>
<meta property="article:tag" content="math"/>
<meta property="article:tag" content="statistics"/>
<meta property="article:tag" content="ml"/>
<meta property="og:image" content="">

  <title>Piddling Pertinent &ndash; SVD v/s MDS v/s PCA</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="./theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href="."></a></h1>

<p>trivially relevant</p>
      <nav>
        <ul class="list">

          <li><a href="http://www.saket-choudhary.me/usc-math-505A-screening-solutions/" target="_blank">Probability Screening Solutions</a></li>
          <li><a href="http://www.saket-choudhary.me/usc-math-541A-screening-solutions/" target="_blank">Stats A Screening Solutions</a></li>
          <li><a href="http://www.saket-choudhary.me/usc-math-541B-screening-solutions/" target="_blank">Stats B Screening Solutions</a></li>
          <li><a href="http://www.saket-choudhary.me/rss-graduate-diploma-solutions" target="_blank">Royal Statistical Society Diploma Solutions</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-twitter" href="https://twitter.com/saketkc" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-github" href="https://github.com/saketkc" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href=".">    Home
</a>

      <a href="http://saket-choudhary.me/pertinent-blog">Home</a>
      <a href="https://github.com/saketkc/pertinent-blog/tree/master/content">Github</a>

      <a href="http://saket-choudhary.me/pertinent-blog/feeds/all.atom.xml">    Atom
</a>

      <a href="http://saket-choudhary.me/pertinent-blog/feeds/all.rss.xml">    RSS
</a>
    </nav>

<article class="single">
  <header>
      
    <h1 id="svd-vs-mds-vs-pca">SVD v/s MDS v/s PCA</h1>
    <p>
          Posted on Wed 15 April 2015 in <a href="./category/misc.html">misc</a>


        &#8226; 3 min read
    </p>
  </header>


  <div>
    <p>Principle Component Analysis(PCA) is a relatively more famous
than Singular Value Decomposition(SVD) or Multidimensional Scaling(MDS).
When I was introduced to the latter two, I was utterly confused
trying to figure out what goes in where.</p>
<h2>SVD</h2>
<p>Let <span class="math">\(X_{mxn}\)</span> data matrix. For an easy to relate example, from bioinformatics,
let each row represent a gene, and each column represent single patient.</p>
<p>The rows thus give expression profile of gene across patients while the columns
represent the expression levels of different genes in each person.</p>
<p>Without loss of generality we assume <span class="math">\(m&gt;n\)</span> and <span class="math">\(rank(X)=r &lt;n\)</span></p>
<p>The singular value decomposition of <span class="math">\(X_{mxn}\)</span> is then given by:</p>
<div class="math">$$
X_{mxn} = U_{mxr}\sum_{rxr}V_{nxr}^T
$$</div>
<p><span class="math">\(U_{nxr}\)</span> consists of orthornormal eigen vectors of <span class="math">\(X^TX\)</span>(nxn)</p>
<p><span class="math">\(V_{mxr}\)</span> consists of orthornormal eigen vectors of <span class="math">\(XX^T\)</span>(mxm)</p>
<p><span class="math">\(\sum_{rxr}\)</span> denotes a diagonal matrix composed of eigen vectors of <span class="math">\(X^TX\)</span> arranged with the largest being located
at top left, least at bottom right.<span class="math">\(\sum_{ii} = \lambda_i\)</span> and <span class="math">\(\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \ldots \geq \lambda_r\)</span></p>
<p>Facts:</p>
<ul>
<li><span class="math">\(\lambda_i \geq 0\)</span> because <span class="math">\(X^TX\)</span> is a positive definite matrix: <span class="math">\(yX^TXy = (Xy)(Xy)^T &gt; 0\)</span> always [See: http://en.wikipedia.org/wiki/Positive-definite_matrix#Characterizations]</li>
</ul>
<h2>PCA</h2>
<p>Let <span class="math">\(X_c\)</span> denote the mean centered <span class="math">\(X\)</span>, i.e. <span class="math">\(X_c = I’_{mxm}X_{mxn}\)</span>
 where <span class="math">\(I’ = I-\frac{1}{m}\)</span> thus from each row we substract the ‘average row’ leading to $X_c4 being zer-mean centerered.</p>
<p>Def: Gram matrix : <span class="math">\(X_cX_c^T\)</span></p>
<p>Def: Covariance Matrix: <span class="math">\(X_c^TX_c\)</span></p>
<p>Let’s say you are given the original uncente</p>
<p>The most ‘common’ definition of PCA says : For a given set of <span class="math">\(n\)</span> dimensional vectors
<span class="math">\(x_1, x_2, x_3,x_4, \ldots x_m\)</span> the <span class="math">\(p&lt;n\)</span> principal components are those orthogonal
axes onto which the variance retained is maximized</p>
<p>and hence </p>
<p>Plotting/Running PCA
- Calculate <span class="math">\(X_cX_c^T\)</span> and calculate <span class="math">\(U\)</span> as the eigen vectors of this matrix. Retain the vectors corresponding
to top <span class="math">\(p=2\)</span> eigenvalues
-  Calculate project <span class="math">\(Y=U^TX\)</span></p>
<p>Given gram matrix of original data <span class="math">\(K\)</span>, to obtain gram matrix of mean centered data, cholesky decomposition is not required:</p>
<p><span class="math">\(K_c = (I-\frac{1}{n})K(I-\frac{1}{n}) = K-\frac{I}{n}K -K\frac{1}{n} \frac{I}{n}K\frac{I}{n}\)</span></p>
<p>Also realize:</p>
<p><span class="math">\(X_c = U \sum V^T\)</span></p>
<p>and <span class="math">\(UU^T=I\)</span> ; <span class="math">\(VV^T=I\)</span></p>
<p>Thus, <span class="math">\(K_c=X_cX_c^T = U\sum^2U^T\)</span></p>
<p>PCA is equivalent of doing a SVD: of <span class="math">\(X_c = U\sumV^T\)</span> and then using <span class="math">\(U\sum\)</span> as the principle components</p>
<h2>MDS</h2>
<p>Given distance matrix <span class="math">\(D_{ij}\)</span> of pairwise distances, what would PCA result into?
Assuming the distances were euclidean:</p>
<div class="math">$$
D_{ij}^2 = ||x_i-x_j||^2 = ||x_i - \overline{x} + \overline{x} -x_j||^2 = ||x_i-\overline{x}||^2 + ||x_j - \overline{x}|^2 -2\langle x_i-\overline{x}, x_j-\overline{x}\rangle
= ||x_i-\overline{x}||^2 + ||x_j - \overline{x}|^2 + 2[K_c]_{ij}
$$</div>
<p>
which can be cleverly rewritten as:
</p>
<div class="math">$$K_c = (I-\frac{1}{n})\frac{-D^2}{2}(I-\frac{1}{n}$$</div>
<p><span class="math">\(K_c\)</span> can now undergo SVD to obtain principal components. which is what happens in <a href="MDS.md">MDS</a> too.</p>
<h2>References</h2>
<p>[1] http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional/132731#132731</p>
<p>[2] SVD: http://infolab.stanford.edu/~ullman/mmds/ch11.pdf</p>
<p>[3] SVD in R: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition</p>
<p>[4] Multidimensional Scaling, Patrick J.F. Groenen∗ Michel van de Velden</p>
<p>[5] SVD and PCA: http://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca</p>
<p>[6] Intuitive Explaination of PCA: http://arxiv.org/pdf/1404.1100.pdf</p>
<p>[7] PCA: http://www3.cs.stonybrook.edu/~sael/teaching/cse549/Slides/CSE549_16.pdf</p>
<p>[8] PCA: http://www.math.ucsd.edu/~gptesler/283/slides/pca_f13-handout.pdf</p>
<p>[9] SVD: http://www.cs.wustl.edu/~zhang/teaching/cs517/Spring12/CourseProjects/SVD.pdf</p>
<p>[10] PCA v/s MDS: http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional</p>
<p>[11] Gaussian Lernels: https://shapeofdata.wordpress.com/2013/07/23/gaussian-kernels/</p>
<p>[12] Kernel PCA: http://sebastianraschka.com/Articles/2014_kernel_pca.html</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/math.html">math</a>
      <a href="./tag/statistics.html">statistics</a>
      <a href="./tag/ml.html">ml</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  2018</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Piddling Pertinent ",
  "url" : ".",
  "image": "",
  "description": "Musings"
}
</script>

</body>
</html>